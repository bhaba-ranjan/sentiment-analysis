{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173cbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e49396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a9fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3880692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\",names=[\"sentiments\", \"reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c00c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(data_frame):\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.lower())\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.translate(str.maketrans('', '', string.punctuation)))\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.translate(str.maketrans('', '', string.digits)))\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: re.sub(\"r[^a-z]\",'',review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e046e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cleaning(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68be92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_data(data_frame):\n",
    "    data_frame['words'] = data_frame.reviews.apply(lambda review: nltk.word_tokenize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc46b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "666a90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(list):\n",
    "    stop_words_removed = []\n",
    "    for i in list:\n",
    "        if i not in stopwords:\n",
    "            stop_words_removed.append(i)\n",
    "    return stop_words_removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5074d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['stop_words_cleaned'] = train_data.words.apply(lambda word_list: remove_stopwords(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc2ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')\n",
    "def tag_pos(list_of_words):\n",
    "    return nltk.pos_tag(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5c0dd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction of lemma words after taggin with pos\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "def find_lemma_word(word):\n",
    "    lemma_words=[]\n",
    "    words_with_pos = tag_pos(word)\n",
    "    for word in words_with_pos:\n",
    "        if word[1].startswith('NN'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='n'))\n",
    "        elif word[1].startswith('VB'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='v'))\n",
    "        elif word[1].startswith('JJ'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='a'))\n",
    "        elif word[1].startswith('RB'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='r'))\n",
    "        else:\n",
    "            lemma_words.append(word[0])\n",
    "            \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fc8a3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['lemma_word'] = train_data.stop_words_cleaned.apply(lambda word_list: find_lemma_word(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "57f99f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cleaned_review'] = train_data.lemma_word.apply(lambda review_list: \" \".join(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ff6e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1477a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_best = SelectKBest(score_func=chi2, k = int(len(selected_vocab)*.20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06afaa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=5524, score_func=<function chi2 at 0x7fcd7625f3b0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sentiments = np.array(train_data['sentiments'])\n",
    "select_best.fit(selected_array,np_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb9c673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5524,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = select_best.get_support()\n",
    "x = selected_tf.get_feature_names_out()\n",
    "y = x[mask]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb633a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_review, test_review, train_sentiment, test_sentiment =train_test_split(train_data.cleaned_review,train_data.sentiments,shuffle=True,random_state=0,stratify=train_data.sentiments,train_size=.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5db58b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5524\n",
      "5524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = global_tf.vocabulary_\n",
    "print(len(y))\n",
    "for i in y:\n",
    "    if vocab_dict[i] == 0:\n",
    "        y.remove(i)\n",
    "print(len(y))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "76a12ab7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'bad possible customeservice experience lv terrible sale associate waste youtime go scottsdale location'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/17dx1py559xc12bhjgp9w4_80000gn/T/ipykernel_9810/4157824810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mselect_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m.20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mselect_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_review\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_sentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \"\"\"\n\u001b[1;32m    397\u001b[0m         X, y = self._validate_data(\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         )\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    855\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    856\u001b[0m         \"\"\"\n\u001b[0;32m--> 857\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'bad possible customeservice experience lv terrible sale associate waste youtime go scottsdale location'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "select_best = SelectKBest(score_func=chi2, k = int(len(selected_vocab)*.20) )\n",
    "np_sentiments = np.array(train_sentiment)\n",
    "select_best.fit(train_review,np_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1eac9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_features = count_vectorizer.fit_transform(train_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a837b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_feat_array = initial_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cd7912f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_top_features = TfidfVectorizer(use_idf=False, vocabulary=y,sublinear_tf=True, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "23616914",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_matrix = count_vectorizer_top_features.fit_transform(train_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a95ea829",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_array = selected_feature_matrix.toarray()\n",
    "selected_test = count_vectorizer_top_features.transform(test_review)\n",
    "selected_test_features = selected_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c9121bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(selected_feature_array,columns=count_vectorizer_top_features.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "abd821da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc': 0,\n",
       " 'abomination': 1,\n",
       " 'aboveaverage': 2,\n",
       " 'abrupt': 3,\n",
       " 'abruptly': 4,\n",
       " 'absurd': 5,\n",
       " 'abuse': 6,\n",
       " 'abysmal': 7,\n",
       " 'ac': 8,\n",
       " 'accept': 9,\n",
       " 'acceptable': 10,\n",
       " 'accessory': 11,\n",
       " 'accommodate': 12,\n",
       " 'accomodating': 13,\n",
       " 'accost': 14,\n",
       " 'account': 15,\n",
       " 'accountability': 16,\n",
       " 'acct': 17,\n",
       " 'accuracy': 18,\n",
       " 'accusatory': 19,\n",
       " 'accuse': 20,\n",
       " 'accutemp': 21,\n",
       " 'ace': 22,\n",
       " 'ache': 23,\n",
       " 'acid': 24,\n",
       " 'acknowledge': 25,\n",
       " 'acknowledgement': 26,\n",
       " 'acknowledgment': 27,\n",
       " 'across': 28,\n",
       " 'act': 29,\n",
       " 'acted': 30,\n",
       " 'activate': 31,\n",
       " 'active': 32,\n",
       " 'activity': 33,\n",
       " 'actual': 34,\n",
       " 'actually': 35,\n",
       " 'acura': 36,\n",
       " 'addict': 37,\n",
       " 'addicted': 38,\n",
       " 'addiction': 39,\n",
       " 'addictive': 40,\n",
       " 'additional': 41,\n",
       " 'additionally': 42,\n",
       " 'address': 43,\n",
       " 'adequately': 44,\n",
       " 'adjust': 45,\n",
       " 'adobada': 46,\n",
       " 'adorable': 47,\n",
       " 'adore': 48,\n",
       " 'adovada': 49,\n",
       " 'advantage': 50,\n",
       " 'advertise': 51,\n",
       " 'advertised': 52,\n",
       " 'advertises': 53,\n",
       " 'advertising': 54,\n",
       " 'advise': 55,\n",
       " 'aec': 56,\n",
       " 'afford': 57,\n",
       " 'affordable': 58,\n",
       " 'aficionado': 59,\n",
       " 'afloat': 60,\n",
       " 'aforementioned': 61,\n",
       " 'african': 62,\n",
       " 'afte': 63,\n",
       " 'aftea': 64,\n",
       " 'afteabout': 65,\n",
       " 'afteall': 66,\n",
       " 'afteanothe': 67,\n",
       " 'afteasking': 68,\n",
       " 'aftebeing': 69,\n",
       " 'aftebringing': 70,\n",
       " 'aftecalling': 71,\n",
       " 'aftechecking': 72,\n",
       " 'aftedinneand': 73,\n",
       " 'aftedriving': 74,\n",
       " 'afteeating': 75,\n",
       " 'afteexplaining': 76,\n",
       " 'aftefinishing': 77,\n",
       " 'aftefive': 78,\n",
       " 'afteflagging': 79,\n",
       " 'aftehaving': 80,\n",
       " 'aftehe': 81,\n",
       " 'aftehearing': 82,\n",
       " 'aftei': 83,\n",
       " 'afteit': 84,\n",
       " 'aftelunch': 85,\n",
       " 'afteme': 86,\n",
       " 'aftemy': 87,\n",
       " 'aftenearly': 88,\n",
       " 'afteone': 89,\n",
       " 'afteordering': 90,\n",
       " 'afteoufood': 91,\n",
       " 'afteouvisit': 92,\n",
       " 'aftepaying': 93,\n",
       " 'aftepm': 94,\n",
       " 'afteseeing': 95,\n",
       " 'afteseveral': 96,\n",
       " 'afteshe': 97,\n",
       " 'aftesitting': 98,\n",
       " 'aftespending': 99,\n",
       " 'aftestanding': 100,\n",
       " 'aftetaking': 101,\n",
       " 'aftetasting': 102,\n",
       " 'aftetax': 103,\n",
       " 'aftethat': 104,\n",
       " 'aftethe': 105,\n",
       " 'aftethey': 106,\n",
       " 'aftethis': 107,\n",
       " 'aftetrying': 108,\n",
       " 'aftetwo': 109,\n",
       " 'afteus': 110,\n",
       " 'aftewaiting': 111,\n",
       " 'aftewatching': 112,\n",
       " 'aftewe': 113,\n",
       " 'aftewhat': 114,\n",
       " 'afteworking': 115,\n",
       " 'againbut': 116,\n",
       " 'againnni': 117,\n",
       " 'againnnmy': 118,\n",
       " 'againnnthe': 119,\n",
       " 'agave': 120,\n",
       " 'age': 121,\n",
       " 'agedashi': 122,\n",
       " 'agent': 123,\n",
       " 'aggravate': 124,\n",
       " 'aggravation': 125,\n",
       " 'aggressive': 126,\n",
       " 'aggressively': 127,\n",
       " 'ago': 128,\n",
       " 'agree': 129,\n",
       " 'agreement': 130,\n",
       " 'agua': 131,\n",
       " 'ahole': 132,\n",
       " 'ahwatukee': 133,\n",
       " 'aiand': 134,\n",
       " 'aiconditioning': 135,\n",
       " 'aihockey': 136,\n",
       " 'aiout': 137,\n",
       " 'aircraft': 138,\n",
       " 'airport': 139,\n",
       " 'airsoft': 140,\n",
       " 'airy': 141,\n",
       " 'ak': 142,\n",
       " 'al': 143,\n",
       " 'alarm': 144,\n",
       " 'albacore': 145,\n",
       " 'albondigas': 146,\n",
       " 'album': 147,\n",
       " 'alcohol': 148,\n",
       " 'alcoholic': 149,\n",
       " 'aldi': 150,\n",
       " 'alex': 151,\n",
       " 'alexander': 152,\n",
       " 'alfredo': 153,\n",
       " 'ali': 154,\n",
       " 'alien': 155,\n",
       " 'alienate': 156,\n",
       " 'alignment': 157,\n",
       " 'alike': 158,\n",
       " 'allege': 159,\n",
       " 'allegedly': 160,\n",
       " 'allegheny': 161,\n",
       " 'allow': 162,\n",
       " 'almost': 163,\n",
       " 'aloha': 164,\n",
       " 'aloof': 165,\n",
       " 'already': 166,\n",
       " 'alright': 167,\n",
       " 'also': 168,\n",
       " 'alta': 169,\n",
       " 'altima': 170,\n",
       " 'always': 171,\n",
       " 'alwun': 172,\n",
       " 'amaze': 173,\n",
       " 'amazeballs': 174,\n",
       " 'amazing': 175,\n",
       " 'amazingly': 176,\n",
       " 'amazingnni': 177,\n",
       " 'amazon': 178,\n",
       " 'ambiance': 179,\n",
       " 'ambience': 180,\n",
       " 'amelia': 181,\n",
       " 'america': 182,\n",
       " 'americanize': 183,\n",
       " 'americas': 184,\n",
       " 'ammo': 185,\n",
       " 'amount': 186,\n",
       " 'ample': 187,\n",
       " 'amtrak': 188,\n",
       " 'amy': 189,\n",
       " 'angel': 190,\n",
       " 'angrily': 191,\n",
       " 'angry': 192,\n",
       " 'angulo': 193,\n",
       " 'animal': 194,\n",
       " 'anna': 195,\n",
       " 'anniversary': 196,\n",
       " 'annoy': 197,\n",
       " 'annoyed': 198,\n",
       " 'anns': 199,\n",
       " 'anothe': 200,\n",
       " 'anotheand': 201,\n",
       " 'anothechance': 202,\n",
       " 'anothecompany': 203,\n",
       " 'anothedrink': 204,\n",
       " 'anotheemployee': 205,\n",
       " 'anothefive': 206,\n",
       " 'anothegirl': 207,\n",
       " 'anothego': 208,\n",
       " 'anothegreat': 209,\n",
       " 'anothegroup': 210,\n",
       " 'anotheguest': 211,\n",
       " 'anothehotel': 212,\n",
       " 'anotheone': 213,\n",
       " 'anotheperson': 214,\n",
       " 'anotheplace': 215,\n",
       " 'anotheplus': 216,\n",
       " 'anotherestaurant': 217,\n",
       " 'anothereviewesaid': 218,\n",
       " 'anotheshot': 219,\n",
       " 'anothestate': 220,\n",
       " 'anothestore': 221,\n",
       " 'anothestory': 222,\n",
       " 'anothetable': 223,\n",
       " 'anotheten': 224,\n",
       " 'anothetry': 225,\n",
       " 'anothewaitress': 226,\n",
       " 'anotheweek': 227,\n",
       " 'anothewoman': 228,\n",
       " 'anotheworld': 229,\n",
       " 'answeall': 230,\n",
       " 'answeand': 231,\n",
       " 'answeas': 232,\n",
       " 'answer': 233,\n",
       " 'answewas': 234,\n",
       " 'anti': 235,\n",
       " 'antibiotic': 236,\n",
       " 'anticipation': 237,\n",
       " 'antiquate': 238,\n",
       " 'antique': 239,\n",
       " 'anymore': 240,\n",
       " 'anyone': 241,\n",
       " 'anything': 242,\n",
       " 'anyway': 243,\n",
       " 'ap': 244,\n",
       " 'apache': 245,\n",
       " 'apart': 246,\n",
       " 'apartment': 247,\n",
       " 'apathetic': 248,\n",
       " 'apathy': 249,\n",
       " 'apiece': 250,\n",
       " 'apologetic': 251,\n",
       " 'apologize': 252,\n",
       " 'apologized': 253,\n",
       " 'apologizes': 254,\n",
       " 'apology': 255,\n",
       " 'appal': 256,\n",
       " 'appalled': 257,\n",
       " 'apparent': 258,\n",
       " 'apparently': 259,\n",
       " 'appeal': 260,\n",
       " 'appear': 261,\n",
       " 'appearance': 262,\n",
       " 'appeared': 263,\n",
       " 'appease': 264,\n",
       " 'appeathat': 265,\n",
       " 'appeato': 266,\n",
       " 'appetizewas': 267,\n",
       " 'appetizing': 268,\n",
       " 'applebees': 269,\n",
       " 'appliance': 270,\n",
       " 'apply': 271,\n",
       " 'appointment': 272,\n",
       " 'appreciate': 273,\n",
       " 'appreciative': 274,\n",
       " 'approx': 275,\n",
       " 'approximately': 276,\n",
       " 'appt': 277,\n",
       " 'april': 278,\n",
       " 'apron': 279,\n",
       " 'aquavina': 280,\n",
       " 'arbys': 281,\n",
       " 'arcade': 282,\n",
       " 'arcadia': 283,\n",
       " 'arch': 284,\n",
       " 'architecture': 285,\n",
       " 'arent': 286,\n",
       " 'argue': 287,\n",
       " 'argument': 288,\n",
       " 'argumentative': 289,\n",
       " 'arizona': 290,\n",
       " 'arm': 291,\n",
       " 'around': 292,\n",
       " 'arrive': 293,\n",
       " 'arrived': 294,\n",
       " 'arrogance': 295,\n",
       " 'arrogant': 296,\n",
       " 'art': 297,\n",
       " 'artichoke': 298,\n",
       " 'as': 299,\n",
       " 'asada': 300,\n",
       " 'asc': 301,\n",
       " 'ash': 302,\n",
       " 'ashamed': 303,\n",
       " 'ashtray': 304,\n",
       " 'aside': 305,\n",
       " 'asinine': 306,\n",
       " 'ask': 307,\n",
       " 'asked': 308,\n",
       " 'aspect': 309,\n",
       " 'assault': 310,\n",
       " 'assertive': 311,\n",
       " 'asshole': 312,\n",
       " 'assign': 313,\n",
       " 'assistance': 314,\n",
       " 'associate': 315,\n",
       " 'assume': 316,\n",
       " 'assumption': 317,\n",
       " 'assure': 318,\n",
       " 'asu': 319,\n",
       " 'ate': 320,\n",
       " 'atlanta': 321,\n",
       " 'atm': 322,\n",
       " 'atmosphere': 323,\n",
       " 'atrias': 324,\n",
       " 'atrium': 325,\n",
       " 'atrocious': 326,\n",
       " 'att': 327,\n",
       " 'attach': 328,\n",
       " 'attack': 329,\n",
       " 'attempt': 330,\n",
       " 'attendant': 331,\n",
       " 'attention': 332,\n",
       " 'attentive': 333,\n",
       " 'attic': 334,\n",
       " 'attitude': 335,\n",
       " 'attorney': 336,\n",
       " 'attract': 337,\n",
       " 'audacity': 338,\n",
       " 'audi': 339,\n",
       " 'audible': 340,\n",
       " 'august': 341,\n",
       " 'authentic': 342,\n",
       " 'authority': 343,\n",
       " 'auto': 344,\n",
       " 'autograph': 345,\n",
       " 'automotive': 346,\n",
       " 'avail': 347,\n",
       " 'avanti': 348,\n",
       " 'ave': 349,\n",
       " 'aveda': 350,\n",
       " 'average': 351,\n",
       " 'avocado': 352,\n",
       " 'avoid': 353,\n",
       " 'aw': 354,\n",
       " 'aware': 355,\n",
       " 'away': 356,\n",
       " 'awe': 357,\n",
       " 'aweful': 358,\n",
       " 'awesome': 359,\n",
       " 'awesomeness': 360,\n",
       " 'awful': 361,\n",
       " 'awfully': 362,\n",
       " 'awfulnni': 363,\n",
       " 'awkward': 364,\n",
       " 'awkwardly': 365,\n",
       " 'awkwardness': 366,\n",
       " 'awning': 367,\n",
       " 'awsome': 368,\n",
       " 'ayce': 369,\n",
       " 'az': 370,\n",
       " 'ba': 371,\n",
       " 'baand': 372,\n",
       " 'babe': 373,\n",
       " 'back': 374,\n",
       " 'backni': 375,\n",
       " 'backnnthis': 376,\n",
       " 'backseat': 377,\n",
       " 'bacrawl': 378,\n",
       " 'bacteria': 379,\n",
       " 'bacterial': 380,\n",
       " 'bad': 381,\n",
       " 'badly': 382,\n",
       " 'bados': 383,\n",
       " 'baffle': 384,\n",
       " 'bag': 385,\n",
       " 'bahad': 386,\n",
       " 'bahas': 387,\n",
       " 'bait': 388,\n",
       " 'baker': 389,\n",
       " 'balcony': 390,\n",
       " 'ball': 391,\n",
       " 'ballantyne': 392,\n",
       " 'ballet': 393,\n",
       " 'balooked': 394,\n",
       " 'balouie': 395,\n",
       " 'baltimore': 396,\n",
       " 'banext': 397,\n",
       " 'banfield': 398,\n",
       " 'bang': 399,\n",
       " 'banh': 400,\n",
       " 'bank': 401,\n",
       " 'banone': 402,\n",
       " 'banot': 403,\n",
       " 'baon': 404,\n",
       " 'bar': 405,\n",
       " 'barb': 406,\n",
       " 'barbecue': 407,\n",
       " 'barber': 408,\n",
       " 'barbershop': 409,\n",
       " 'barbq': 410,\n",
       " 'barbs': 411,\n",
       " 'bare': 412,\n",
       " 'barely': 413,\n",
       " 'barista': 414,\n",
       " 'baristassales': 415,\n",
       " 'barn': 416,\n",
       " 'barringtons': 417,\n",
       " 'barrio': 418,\n",
       " 'barriques': 419,\n",
       " 'barrymore': 420,\n",
       " 'barstaff': 421,\n",
       " 'bartendegave': 422,\n",
       " 'bartendejust': 423,\n",
       " 'bartendemade': 424,\n",
       " 'bartender': 425,\n",
       " 'bartendesaid': 426,\n",
       " 'bartendeshe': 427,\n",
       " 'bartendeto': 428,\n",
       " 'bartendewere': 429,\n",
       " 'bartendewho': 430,\n",
       " 'bartendeworking': 431,\n",
       " 'bartending': 432,\n",
       " 'base': 433,\n",
       " 'baseating': 434,\n",
       " 'bashe': 435,\n",
       " 'basic': 436,\n",
       " 'basically': 437,\n",
       " 'basket': 438,\n",
       " 'baskin': 439,\n",
       " 'bathan': 440,\n",
       " 'bathroom': 441,\n",
       " 'bathtub': 442,\n",
       " 'bato': 443,\n",
       " 'bawas': 444,\n",
       " 'bawasnt': 445,\n",
       " 'bawhen': 446,\n",
       " 'bawould': 447,\n",
       " 'bbb': 448,\n",
       " 'bbq': 449,\n",
       " 'bc': 450,\n",
       " 'bdpec': 451,\n",
       " 'beach': 452,\n",
       " 'bean': 453,\n",
       " 'bear': 454,\n",
       " 'beat': 455,\n",
       " 'beaten': 456,\n",
       " 'beautiful': 457,\n",
       " 'beautifully': 458,\n",
       " 'beauty': 459,\n",
       " 'become': 460,\n",
       " 'bed': 461,\n",
       " 'bedillions': 462,\n",
       " 'bedroom': 463,\n",
       " 'beeand': 464,\n",
       " 'beebecause': 465,\n",
       " 'beefloat': 466,\n",
       " 'beehe': 467,\n",
       " 'beei': 468,\n",
       " 'beer': 469,\n",
       " 'beeshe': 470,\n",
       " 'beetaps': 471,\n",
       " 'beetle': 472,\n",
       " 'beewas': 473,\n",
       " 'beg': 474,\n",
       " 'begin': 475,\n",
       " 'behalf': 476,\n",
       " 'behind': 477,\n",
       " 'being': 478,\n",
       " 'belay': 479,\n",
       " 'believe': 480,\n",
       " 'belittle': 481,\n",
       " 'bell': 482,\n",
       " 'bellevue': 483,\n",
       " 'belongs': 484,\n",
       " 'bench': 485,\n",
       " 'beneath': 486,\n",
       " 'benefit': 487,\n",
       " 'benihana': 488,\n",
       " 'beppo': 489,\n",
       " 'berate': 490,\n",
       " 'bertos': 491,\n",
       " 'beside': 492,\n",
       " 'best': 493,\n",
       " 'beth': 494,\n",
       " 'betos': 495,\n",
       " 'bette': 496,\n",
       " 'betteand': 497,\n",
       " 'betteat': 498,\n",
       " 'bettebbq': 499,\n",
       " 'betteburgers': 500,\n",
       " 'bettebusiness': 501,\n",
       " 'bettebut': 502,\n",
       " 'bettechicken': 503,\n",
       " 'bettechinese': 504,\n",
       " 'bettechoices': 505,\n",
       " 'bettedays': 506,\n",
       " 'betteelsewhere': 507,\n",
       " 'betteexperience': 508,\n",
       " 'betteexperiences': 509,\n",
       " 'bettefood': 510,\n",
       " 'bettefrom': 511,\n",
       " 'bettein': 512,\n",
       " 'betteit': 513,\n",
       " 'betteits': 514,\n",
       " 'bettejob': 515,\n",
       " 'bettelike': 516,\n",
       " 'betteluck': 517,\n",
       " 'bettemeal': 518,\n",
       " 'betteoff': 519,\n",
       " 'betteoptions': 520,\n",
       " 'betteplaces': 521,\n",
       " 'bettequality': 522,\n",
       " 'betterestaurant': 523,\n",
       " 'betterestaurants': 524,\n",
       " 'betteservice': 525,\n",
       " 'bettespent': 526,\n",
       " 'bettesteak': 527,\n",
       " 'bettethai': 528,\n",
       " 'bettethe': 529,\n",
       " 'beware': 530,\n",
       " 'bfast': 531,\n",
       " 'bh': 532,\n",
       " 'bi': 533,\n",
       " 'bianco': 534,\n",
       " 'bibimbap': 535,\n",
       " 'bid': 536,\n",
       " 'bighorn': 537,\n",
       " 'bike': 538,\n",
       " 'bikers': 539,\n",
       " 'bill': 540,\n",
       " 'billing': 541,\n",
       " 'biryani': 542,\n",
       " 'bishop': 543,\n",
       " 'bitch': 544,\n",
       " 'bitchy': 545,\n",
       " 'bite': 546,\n",
       " 'bitesized': 547,\n",
       " 'bittetaste': 548,\n",
       " 'bizarre': 549,\n",
       " 'bk': 550,\n",
       " 'blacken': 551,\n",
       " 'blackout': 552,\n",
       " 'blah': 553,\n",
       " 'blame': 554,\n",
       " 'bland': 555,\n",
       " 'blandness': 556,\n",
       " 'blank': 557,\n",
       " 'blankly': 558,\n",
       " 'blasphemy': 559,\n",
       " 'blast': 560,\n",
       " 'blatantly': 561,\n",
       " 'blech': 562,\n",
       " 'blend': 563,\n",
       " 'blimpie': 564,\n",
       " 'blob': 565,\n",
       " 'block': 566,\n",
       " 'blonde': 567,\n",
       " 'blood': 568,\n",
       " 'bloodwork': 569,\n",
       " 'bloody': 570,\n",
       " 'bloom': 571,\n",
       " 'bloomfield': 572,\n",
       " 'bloomington': 573,\n",
       " 'blow': 574,\n",
       " 'boa': 575,\n",
       " 'boasausage': 576,\n",
       " 'bob': 577,\n",
       " 'boba': 578,\n",
       " 'bobbie': 579,\n",
       " 'bobby': 580,\n",
       " 'bofa': 581,\n",
       " 'boil': 582,\n",
       " 'bojangles': 583,\n",
       " 'bomb': 584,\n",
       " 'bombay': 585,\n",
       " 'bomberro': 586,\n",
       " 'bone': 587,\n",
       " 'bonus': 588,\n",
       " 'boo': 589,\n",
       " 'booby': 590,\n",
       " 'booked': 591,\n",
       " 'bookman': 592,\n",
       " 'bookmarked': 593,\n",
       " 'booooo': 594,\n",
       " 'borderline': 595,\n",
       " 'bore': 596,\n",
       " 'boring': 597,\n",
       " 'botanical': 598,\n",
       " 'bothe': 599,\n",
       " 'bothecoming': 600,\n",
       " 'bother': 601,\n",
       " 'botheto': 602,\n",
       " 'bothewith': 603,\n",
       " 'bottom': 604,\n",
       " 'bougainvillea': 605,\n",
       " 'bought': 606,\n",
       " 'bouldering': 607,\n",
       " 'bourbon': 608,\n",
       " 'boutique': 609,\n",
       " 'bowl': 610,\n",
       " 'box': 611,\n",
       " 'boyardee': 612,\n",
       " 'boycott': 613,\n",
       " 'boyfriend': 614,\n",
       " 'brag': 615,\n",
       " 'bran': 616,\n",
       " 'brand': 617,\n",
       " 'bravo': 618,\n",
       " 'bread': 619,\n",
       " 'breadcrafters': 620,\n",
       " 'breadstick': 621,\n",
       " 'break': 622,\n",
       " 'breakfast': 623,\n",
       " 'breast': 624,\n",
       " 'breathtaking': 625,\n",
       " 'breed': 626,\n",
       " 'breeze': 627,\n",
       " 'brennans': 628,\n",
       " 'brent': 629,\n",
       " 'brew': 630,\n",
       " 'brian': 631,\n",
       " 'bride': 632,\n",
       " 'brie': 633,\n",
       " 'brigetts': 634,\n",
       " 'bright': 635,\n",
       " 'brilliant': 636,\n",
       " 'bring': 637,\n",
       " 'bringing': 638,\n",
       " 'brings': 639,\n",
       " 'brioche': 640,\n",
       " 'brittany': 641,\n",
       " 'brixx': 642,\n",
       " 'bro': 643,\n",
       " 'broasted': 644,\n",
       " 'broccoli': 645,\n",
       " 'brocolli': 646,\n",
       " 'broken': 647,\n",
       " 'bros': 648,\n",
       " 'broth': 649,\n",
       " 'brother': 650,\n",
       " 'brought': 651,\n",
       " 'brown': 652,\n",
       " 'browse': 653,\n",
       " 'bruce': 654,\n",
       " 'bruchetta': 655,\n",
       " 'brueggers': 656,\n",
       " 'brugge': 657,\n",
       " 'brulee': 658,\n",
       " 'brunette': 659,\n",
       " 'bruschetta': 660,\n",
       " 'bruschettas': 661,\n",
       " 'brush': 662,\n",
       " 'brushetta': 663,\n",
       " 'brussels': 664,\n",
       " 'bs': 665,\n",
       " 'bubba': 666,\n",
       " 'buca': 667,\n",
       " 'buck': 668,\n",
       " 'bucket': 669,\n",
       " 'budge': 670,\n",
       " 'budget': 671,\n",
       " 'buds': 672,\n",
       " 'bueno': 673,\n",
       " 'buffalo': 674,\n",
       " 'buffet': 675,\n",
       " 'bug': 676,\n",
       " 'building': 677,\n",
       " 'bulkogi': 678,\n",
       " 'bullshit': 679,\n",
       " 'bully': 680,\n",
       " 'bum': 681,\n",
       " 'bumpeboats': 682,\n",
       " 'bun': 683,\n",
       " 'bunch': 684,\n",
       " 'bundt': 685,\n",
       " 'buonos': 686,\n",
       " 'burden': 687,\n",
       " 'bureau': 688,\n",
       " 'burgatory': 689,\n",
       " 'burgecame': 690,\n",
       " 'burgein': 691,\n",
       " 'burgeking': 692,\n",
       " 'burgethat': 693,\n",
       " 'burgethe': 694,\n",
       " 'burgewas': 695,\n",
       " 'burgewhich': 696,\n",
       " 'burgh': 697,\n",
       " 'burland': 698,\n",
       " 'burn': 699,\n",
       " 'burnt': 700,\n",
       " 'burrata': 701,\n",
       " 'burrito': 702,\n",
       " 'burritos': 703,\n",
       " 'burro': 704,\n",
       " 'bury': 705,\n",
       " 'bus': 706,\n",
       " 'busiethan': 707,\n",
       " 'business': 708,\n",
       " 'buss': 709,\n",
       " 'bust': 710,\n",
       " 'busy': 711,\n",
       " 'buttei': 712,\n",
       " 'butteis': 713,\n",
       " 'butteon': 714,\n",
       " 'buttercream': 715,\n",
       " 'butterfly': 716,\n",
       " 'butternut': 717,\n",
       " 'buttery': 718,\n",
       " 'buttewas': 719,\n",
       " 'button': 720,\n",
       " 'buvons': 721,\n",
       " 'buy': 722,\n",
       " 'buyebeware': 723,\n",
       " 'byob': 724,\n",
       " 'caand': 725,\n",
       " 'caas': 726,\n",
       " 'cab': 727,\n",
       " 'cabeza': 728,\n",
       " 'cabinet': 729,\n",
       " 'cable': 730,\n",
       " 'cacti': 731,\n",
       " 'cactus': 732,\n",
       " 'caddy': 733,\n",
       " 'cadealership': 734,\n",
       " 'cadetailed': 735,\n",
       " 'cadillac': 736,\n",
       " 'caesasalad': 737,\n",
       " 'cafe': 738,\n",
       " 'cafeteria': 739,\n",
       " 'cafoa': 740,\n",
       " 'cage': 741,\n",
       " 'cahad': 742,\n",
       " 'caite': 743,\n",
       " 'cajun': 744,\n",
       " 'cake': 745,\n",
       " 'cal': 746,\n",
       " 'caliente': 747,\n",
       " 'californian': 748,\n",
       " 'call': 749,\n",
       " 'calmly': 750,\n",
       " 'calooks': 751,\n",
       " 'caloric': 752,\n",
       " 'calypso': 753,\n",
       " 'calzone': 754,\n",
       " 'camelback': 755,\n",
       " 'campaign': 756,\n",
       " 'campus': 757,\n",
       " 'can': 758,\n",
       " 'cancel': 759,\n",
       " 'cancellation': 760,\n",
       " 'cancelled': 761,\n",
       " 'candidate': 762,\n",
       " 'candy': 763,\n",
       " 'canyon': 764,\n",
       " 'capable': 765,\n",
       " 'capellini': 766,\n",
       " 'caprese': 767,\n",
       " 'capri': 768,\n",
       " 'caramel': 769,\n",
       " 'carb': 770,\n",
       " 'card': 771,\n",
       " 'cardboard': 772,\n",
       " 'cardio': 773,\n",
       " 'care': 774,\n",
       " 'careless': 775,\n",
       " 'carepairs': 776,\n",
       " 'caribbean': 777,\n",
       " 'carin': 778,\n",
       " 'carissa': 779,\n",
       " 'carl': 780,\n",
       " 'carlos': 781,\n",
       " 'carmons': 782,\n",
       " 'carne': 783,\n",
       " 'carnitas': 784,\n",
       " 'carolina': 785,\n",
       " 'carolinas': 786,\n",
       " 'carolyn': 787,\n",
       " 'carowinds': 788,\n",
       " 'carpet': 789,\n",
       " 'carpeting': 790,\n",
       " 'carry': 791,\n",
       " 'carton': 792,\n",
       " 'carwash': 793,\n",
       " 'casa': 794,\n",
       " 'casbah': 795,\n",
       " 'case': 796,\n",
       " 'caseats': 797,\n",
       " 'cash': 798,\n",
       " 'cashieand': 799,\n",
       " 'cashiei': 800,\n",
       " 'cashiesaid': 801,\n",
       " 'cashieshe': 802,\n",
       " 'cashiewas': 803,\n",
       " 'cashiewho': 804,\n",
       " 'cashmere': 805,\n",
       " 'casita': 806,\n",
       " 'casserole': 807,\n",
       " 'castle': 808,\n",
       " 'casual': 809,\n",
       " 'caught': 810,\n",
       " 'caulifloweand': 811,\n",
       " 'cause': 812,\n",
       " 'caution': 813,\n",
       " 'cavity': 814,\n",
       " 'cawas': 815,\n",
       " 'cawash': 816,\n",
       " 'cawasnt': 817,\n",
       " 'cdg': 818,\n",
       " 'cds': 819,\n",
       " 'cedapoint': 820,\n",
       " 'ceiling': 821,\n",
       " 'cellaand': 822,\n",
       " 'cement': 823,\n",
       " 'central': 824,\n",
       " 'centre': 825,\n",
       " 'ceo': 826,\n",
       " 'certain': 827,\n",
       " 'certainly': 828,\n",
       " 'certainty': 829,\n",
       " 'certificate': 830,\n",
       " 'ceviche': 831,\n",
       " 'chain': 832,\n",
       " 'challenge': 833,\n",
       " 'chambana': 834,\n",
       " 'champagne': 835,\n",
       " 'champaign': 836,\n",
       " 'champaignurbana': 837,\n",
       " 'champon': 838,\n",
       " 'chance': 839,\n",
       " 'chandleand': 840,\n",
       " 'chang': 841,\n",
       " 'change': 842,\n",
       " 'changed': 843,\n",
       " 'channel': 844,\n",
       " 'charge': 845,\n",
       " 'charlotte': 846,\n",
       " 'charlottedouglas': 847,\n",
       " 'chase': 848,\n",
       " 'chat': 849,\n",
       " 'cheap': 850,\n",
       " 'cheapest': 851,\n",
       " 'cheapethan': 852,\n",
       " 'cheapethe': 853,\n",
       " 'cheapskate': 854,\n",
       " 'cheba': 855,\n",
       " 'check': 856,\n",
       " 'checkout': 857,\n",
       " 'cheer': 858,\n",
       " 'cheeseburgeand': 859,\n",
       " 'cheeseburgewas': 860,\n",
       " 'cheesesteaks': 861,\n",
       " 'chef': 862,\n",
       " 'cherryblossom': 863,\n",
       " 'chest': 864,\n",
       " 'chew': 865,\n",
       " 'chewy': 866,\n",
       " 'chicago': 867,\n",
       " 'chicken': 868,\n",
       " 'chihuly': 869,\n",
       " 'chiladas': 870,\n",
       " 'childhood': 871,\n",
       " 'chile': 872,\n",
       " 'chili': 873,\n",
       " 'chill': 874,\n",
       " 'chimichanga': 875,\n",
       " 'chin': 876,\n",
       " 'china': 877,\n",
       " 'chipolte': 878,\n",
       " 'chocolate': 879,\n",
       " 'choke': 880,\n",
       " 'chose': 881,\n",
       " 'chow': 882,\n",
       " 'chowdewas': 883,\n",
       " 'christy': 884,\n",
       " 'chrome': 885,\n",
       " 'church': 886,\n",
       " 'ciabatta': 887,\n",
       " 'cigarette': 888,\n",
       " 'cine': 889,\n",
       " 'cinnamon': 890,\n",
       " 'circle': 891,\n",
       " 'circumstance': 892,\n",
       " 'city': 893,\n",
       " 'claim': 894,\n",
       " 'clarion': 895,\n",
       " 'class': 896,\n",
       " 'classic': 897,\n",
       " 'classical': 898,\n",
       " 'cleaning': 899,\n",
       " 'cleanliness': 900,\n",
       " 'cleanse': 901,\n",
       " 'clear': 902,\n",
       " 'clearly': 903,\n",
       " 'cleathat': 904,\n",
       " 'cleathe': 905,\n",
       " 'clerk': 906,\n",
       " 'climb': 907,\n",
       " 'clinic': 908,\n",
       " 'clock': 909,\n",
       " 'close': 910,\n",
       " 'closed': 911,\n",
       " 'closelook': 912,\n",
       " 'closely': 913,\n",
       " 'closing': 914,\n",
       " 'closure': 915,\n",
       " 'clothes': 916,\n",
       " 'clothing': 917,\n",
       " 'clt': 918,\n",
       " 'clue': 919,\n",
       " 'clueless': 920,\n",
       " 'clump': 921,\n",
       " 'cmon': 922,\n",
       " 'coach': 923,\n",
       " 'coarse': 924,\n",
       " 'coat': 925,\n",
       " 'cobb': 926,\n",
       " 'cocina': 927,\n",
       " 'cockroach': 928,\n",
       " 'cocktails': 929,\n",
       " 'cocky': 930,\n",
       " 'coconut': 931,\n",
       " 'code': 932,\n",
       " 'cody': 933,\n",
       " 'coe': 934,\n",
       " 'coes': 935,\n",
       " 'coincidence': 936,\n",
       " 'cold': 937,\n",
       " 'cole': 938,\n",
       " 'coleslaw': 939,\n",
       " 'collard': 940,\n",
       " 'collect': 941,\n",
       " 'college': 942,\n",
       " 'coloand': 943,\n",
       " 'colorful': 944,\n",
       " 'coma': 945,\n",
       " 'combo': 946,\n",
       " 'come': 947,\n",
       " 'comedian': 948,\n",
       " 'comedy': 949,\n",
       " 'comfort': 950,\n",
       " 'comfortable': 951,\n",
       " 'comfy': 952,\n",
       " 'comic': 953,\n",
       " 'coming': 954,\n",
       " 'comment': 955,\n",
       " 'common': 956,\n",
       " 'communicate': 957,\n",
       " 'communication': 958,\n",
       " 'community': 959,\n",
       " 'commutesandwich': 960,\n",
       " 'comp': 961,\n",
       " 'company': 962,\n",
       " 'comparable': 963,\n",
       " 'comparatively': 964,\n",
       " 'compassionate': 965,\n",
       " 'comped': 966,\n",
       " 'compensate': 967,\n",
       " 'compensation': 968,\n",
       " 'competition': 969,\n",
       " 'complain': 970,\n",
       " 'complement': 971,\n",
       " 'complete': 972,\n",
       " 'completely': 973,\n",
       " 'complex': 974,\n",
       " 'compliment': 975,\n",
       " 'comply': 976,\n",
       " 'concentra': 977,\n",
       " 'concept': 978,\n",
       " 'concern': 979,\n",
       " 'conclude': 980,\n",
       " 'conclusion': 981,\n",
       " 'condescend': 982,\n",
       " 'condescending': 983,\n",
       " 'condescendingly': 984,\n",
       " 'condom': 985,\n",
       " 'conduct': 986,\n",
       " 'cone': 987,\n",
       " 'conference': 988,\n",
       " 'confirm': 989,\n",
       " 'confirmation': 990,\n",
       " 'confront': 991,\n",
       " 'confuse': 992,\n",
       " 'confused': 993,\n",
       " 'confusion': 994,\n",
       " 'congeal': 995,\n",
       " 'congealed': 996,\n",
       " 'congregate': 997,\n",
       " 'congress': 998,\n",
       " 'connection': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_top_features.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e937f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in df.columns.values:\n",
    "    arr.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e6b0d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(arr))\n",
    "# arr.remove('sentiments')\n",
    "# len(arr)\n",
    "df['sentiments'] = np.array(train_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "64c848c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abomination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aboveaverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abrupt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abruptly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>zimbrick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>zinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>zoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>zorbas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_variable\n",
       "0                 abc\n",
       "1         abomination\n",
       "2        aboveaverage\n",
       "3              abrupt\n",
       "4            abruptly\n",
       "...               ...\n",
       "5519             zero\n",
       "5520         zimbrick\n",
       "5521            zinho\n",
       "5522              zoo\n",
       "5523           zorbas\n",
       "\n",
       "[5524 rows x 1 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep_chi_table = pd.DataFrame(arr, columns=['feature_variable'])\n",
    "df_prep_chi_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6e3f15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_chi_table['-1'] = df_prep_chi_table.feature_variable.apply(lambda feat: df.loc[df['sentiments']== -1, feat].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f5f4b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_chi_table['+1'] = df_prep_chi_table.feature_variable.apply(lambda feat: df.loc[df['sentiments']== 1, feat].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67f8fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum = df_prep_chi_table['+1'].sum()+df_prep_chi_table['-1'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "227ac6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep_chi_table['sum_total'] = df_prep_chi_table['+1'] + df_prep_chi_table['-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5fef82af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_variable</th>\n",
       "      <th>-1</th>\n",
       "      <th>+1</th>\n",
       "      <th>sum_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.114516</td>\n",
       "      <td>1.114516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abomination</td>\n",
       "      <td>0.861434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aboveaverage</td>\n",
       "      <td>0.500549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abrupt</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abruptly</td>\n",
       "      <td>0.806805</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.885578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>zero</td>\n",
       "      <td>21.956302</td>\n",
       "      <td>2.288448</td>\n",
       "      <td>24.244751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>zimbrick</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.660870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>zinho</td>\n",
       "      <td>0.253482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>zoo</td>\n",
       "      <td>6.566455</td>\n",
       "      <td>14.748020</td>\n",
       "      <td>21.314475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>zorbas</td>\n",
       "      <td>1.097310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.097310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_variable         -1         +1  sum_total\n",
       "0                 abc   0.000000   1.114516   1.114516\n",
       "1         abomination   0.861434   0.000000   0.861434\n",
       "2        aboveaverage   0.500549   0.000000   0.500549\n",
       "3              abrupt   0.524441   0.000000   0.524441\n",
       "4            abruptly   0.806805   0.078773   0.885578\n",
       "...               ...        ...        ...        ...\n",
       "5519             zero  21.956302   2.288448  24.244751\n",
       "5520         zimbrick   0.660870   0.000000   0.660870\n",
       "5521            zinho   0.253482   0.000000   0.253482\n",
       "5522              zoo   6.566455  14.748020  21.314475\n",
       "5523           zorbas   1.097310   0.000000   1.097310\n",
       "\n",
       "[5524 rows x 4 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep_chi_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e8d97de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91881.6506845339"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765788e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ece56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77004b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb86e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16774d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987bd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2ced4378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.114516\n",
       "1        0.861434\n",
       "2        0.500549\n",
       "3        0.524441\n",
       "4        0.885578\n",
       "          ...    \n",
       "5519    24.244751\n",
       "5520     0.660870\n",
       "5521     0.253482\n",
       "5522    21.314475\n",
       "5523     1.097310\n",
       "Name: sum_total, Length: 5524, dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep_chi_table['sum_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b1259585",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_neg_sentiment = np.array(df_prep_chi_table['-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2e626056",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_pos_sentiment = np.array(df_prep_chi_table['+1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3c86f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_row_sum = np.array(df_prep_chi_table['sum_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cb2ebb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_val(x ,y ,row_total, x_total, y_total, sum_total):\n",
    "    exp_x=[]\n",
    "    exp_y=[]\n",
    "    for i in range(0,len(x)):\n",
    "        exp_x.append(row_total[i]*x_total/sum_total)\n",
    "        exp_y.append(row_total[i]*y_total/sum_total)\n",
    "    return exp_x,exp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "db802fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_col_sum = df_prep_chi_table['-1'].sum()\n",
    "pos_col_sum = df_prep_chi_table['+1'].sum()\n",
    "e_neg,e_pos = calculate_expected_val(array_neg_sentiment, array_pos_sentiment, array_row_sum,neg_col_sum ,pos_col_sum, total_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "21829577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a2f83fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp['+1'] = e_pos\n",
    "df_exp['-1'] = e_neg\n",
    "df_chi_value = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4c4c1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi_value['-1'] = ((df_prep_chi_table['-1']- df_exp['-1'])**2)/df_exp['-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c6f9fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi_value['+1'] = ((df_prep_chi_table['+1']- df_exp['+1'])**2)/df_exp['+1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aa138f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>+1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.602859</td>\n",
       "      <td>0.710319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335643</td>\n",
       "      <td>0.395471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195030</td>\n",
       "      <td>0.229794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.240763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224292</td>\n",
       "      <td>0.264272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>5.961384</td>\n",
       "      <td>7.023999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>0.257497</td>\n",
       "      <td>0.303395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.116369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>2.136306</td>\n",
       "      <td>2.517102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>0.427547</td>\n",
       "      <td>0.503758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            -1        +1\n",
       "0     0.602859  0.710319\n",
       "1     0.335643  0.395471\n",
       "2     0.195030  0.229794\n",
       "3     0.204339  0.240763\n",
       "4     0.224292  0.264272\n",
       "...        ...       ...\n",
       "5519  5.961384  7.023999\n",
       "5520  0.257497  0.303395\n",
       "5521  0.098765  0.116369\n",
       "5522  2.136306  2.517102\n",
       "5523  0.427547  0.503758\n",
       "\n",
       "[5524 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c3513e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chi_val = []\n",
    "for row in df_chi_value.itertuples():\n",
    "    max_chi_val.append(max(row[1],row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "689cd637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>+1</th>\n",
       "      <th>max_chi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.602859</td>\n",
       "      <td>0.710319</td>\n",
       "      <td>0.710319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335643</td>\n",
       "      <td>0.395471</td>\n",
       "      <td>0.395471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.195030</td>\n",
       "      <td>0.229794</td>\n",
       "      <td>0.229794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.240763</td>\n",
       "      <td>0.240763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.224292</td>\n",
       "      <td>0.264272</td>\n",
       "      <td>0.264272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>5.961384</td>\n",
       "      <td>7.023999</td>\n",
       "      <td>7.023999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>0.257497</td>\n",
       "      <td>0.303395</td>\n",
       "      <td>0.303395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.116369</td>\n",
       "      <td>0.116369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>2.136306</td>\n",
       "      <td>2.517102</td>\n",
       "      <td>2.517102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>0.427547</td>\n",
       "      <td>0.503758</td>\n",
       "      <td>0.503758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            -1        +1   max_chi\n",
       "0     0.602859  0.710319  0.710319\n",
       "1     0.335643  0.395471  0.395471\n",
       "2     0.195030  0.229794  0.229794\n",
       "3     0.204339  0.240763  0.240763\n",
       "4     0.224292  0.264272  0.264272\n",
       "...        ...       ...       ...\n",
       "5519  5.961384  7.023999  7.023999\n",
       "5520  0.257497  0.303395  0.303395\n",
       "5521  0.098765  0.116369  0.116369\n",
       "5522  2.136306  2.517102  2.517102\n",
       "5523  0.427547  0.503758  0.503758\n",
       "\n",
       "[5524 rows x 3 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chi_value['max_chi'] = max_chi_val\n",
    "df_chi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "069b677a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feature_array.shape\n",
    "np.any(np.isfinite(selected_feature_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3666051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.array(train_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c0410e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = np.array(df_chi_value['+1'])\n",
    "neg_weight = np.array(df_chi_value['-1'])\n",
    "mx_weight = np.array(df_chi_value['max_chi'])\n",
    "\n",
    "def weighted_feature(selected_feature_array, sentiment):\n",
    "    weighted_feat_array = []\n",
    "    ts = np.array(sentiment)\n",
    "    for i in range(0, len(sentiment)):        \n",
    "        if ts[i] == 1:\n",
    "            weighted_feat_array.append(np.multiply(selected_feature_array[i],pos_weight))\n",
    "        else:\n",
    "            weighted_feat_array.append(np.multiply(selected_feature_array[i],neg_weight))\n",
    "    return weighted_feat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "eb8f00de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weighted_feature_train = weighted_feature(selected_feature_array, train_sentiment)\n",
    "# np.any(np.isnan(weighted_feature_train))\n",
    "np.any(np.isnan(neg_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cef1d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_feature_k(selected_feature_array, sentiment):\n",
    "    weighted_feat_array = []\n",
    "    ts = np.array(sentiment)\n",
    "    for i in range(0, len(sentiment)):        \n",
    "        if ts[i] == 1:\n",
    "            weighted_feat_array.append(np.multiply(selected_feature_array[i],mx_weight))\n",
    "        else:\n",
    "            weighted_feat_array.append(np.multiply(selected_feature_array[i],mx_weight))\n",
    "    return weighted_feat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cb06647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_feature_test = weighted_feature_k(selected_test_features, test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2df5e1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isfinite(selected_feature_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bbd0e28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/17dx1py559xc12bhjgp9w4_80000gn/T/ipykernel_9810/330794069.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclassifer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclassifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_feature_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msentiments_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_feature_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiments_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/aenv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "track_precision=[]\n",
    "for i in range(5,21):\n",
    "    print(i)\n",
    "    classifer = KNeighborsClassifier(n_neighbors=14,n_jobs=3,weights='distance')\n",
    "    classifer.fit(weighted_feature_train, train_sentiment)\n",
    "    sentiments_predict = classifer.predict(weighted_feature_test)\n",
    "    score = metrics.accuracy_score(test_sentiment,sentiments_predict)\n",
    "    print(score)\n",
    "    track_precision.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3a86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "35e12ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# outfile = open('results.csv','w')\n",
    "# out = csv.writer(outfile)\n",
    "# out.writerows(map(lambda x: [x], score_predict))\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d17ff5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_df = pd.read_csv('1661892619_9579706_test_file.csv', names=['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a101b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cleaning(result_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a1d99f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_data(result_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c52334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_df['stop_words_cleaned'] = result_test_df.words.apply(lambda word_list: remove_stopwords(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "06877340",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_df['lemma_word'] = result_test_df.stop_words_cleaned.apply(lambda word_list: find_lemma_word(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b3481d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_df['cleaned_review'] = result_test_df.lemma_word.apply(lambda review_list: \" \".join(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "805f2e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>words</th>\n",
       "      <th>stop_words_cleaned</th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>got takeout from here last night and it was ho...</td>\n",
       "      <td>[got, takeout, from, here, last, night, and, i...</td>\n",
       "      <td>[got, takeout, last, night, horrible, somethin...</td>\n",
       "      <td>[get, takeout, last, night, horrible, somethin...</td>\n",
       "      <td>get takeout last night horrible something must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girls are sweet and prices are reasonable the ...</td>\n",
       "      <td>[girls, are, sweet, and, prices, are, reasonab...</td>\n",
       "      <td>[girls, sweet, prices, reasonable, stand, bed,...</td>\n",
       "      <td>[girl, sweet, price, reasonable, stand, bed, h...</td>\n",
       "      <td>girl sweet price reasonable stand bed hot make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rudest people i have eveencountered  husband a...</td>\n",
       "      <td>[rudest, people, i, have, eveencountered, husb...</td>\n",
       "      <td>[rudest, people, eveencountered, husband, wife...</td>\n",
       "      <td>[rudest, people, eveencountered, husband, wife...</td>\n",
       "      <td>rudest people eveencountered husband wife own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this airport is only coveted fothe destination...</td>\n",
       "      <td>[this, airport, is, only, coveted, fothe, dest...</td>\n",
       "      <td>[airport, coveted, fothe, destination, leads, ...</td>\n",
       "      <td>[airport, covet, fothe, destination, lead, fli...</td>\n",
       "      <td>airport covet fothe destination lead flier vie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the last  months have shown a steady decline i...</td>\n",
       "      <td>[the, last, months, have, shown, a, steady, de...</td>\n",
       "      <td>[last, months, shown, steady, decline, pisspoo...</td>\n",
       "      <td>[last, month, show, steady, decline, pisspoose...</td>\n",
       "      <td>last month show steady decline pisspooservice ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0  got takeout from here last night and it was ho...   \n",
       "1  girls are sweet and prices are reasonable the ...   \n",
       "2  rudest people i have eveencountered  husband a...   \n",
       "3  this airport is only coveted fothe destination...   \n",
       "4  the last  months have shown a steady decline i...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [got, takeout, from, here, last, night, and, i...   \n",
       "1  [girls, are, sweet, and, prices, are, reasonab...   \n",
       "2  [rudest, people, i, have, eveencountered, husb...   \n",
       "3  [this, airport, is, only, coveted, fothe, dest...   \n",
       "4  [the, last, months, have, shown, a, steady, de...   \n",
       "\n",
       "                                  stop_words_cleaned  \\\n",
       "0  [got, takeout, last, night, horrible, somethin...   \n",
       "1  [girls, sweet, prices, reasonable, stand, bed,...   \n",
       "2  [rudest, people, eveencountered, husband, wife...   \n",
       "3  [airport, coveted, fothe, destination, leads, ...   \n",
       "4  [last, months, shown, steady, decline, pisspoo...   \n",
       "\n",
       "                                          lemma_word  \\\n",
       "0  [get, takeout, last, night, horrible, somethin...   \n",
       "1  [girl, sweet, price, reasonable, stand, bed, h...   \n",
       "2  [rudest, people, eveencountered, husband, wife...   \n",
       "3  [airport, covet, fothe, destination, lead, fli...   \n",
       "4  [last, month, show, steady, decline, pisspoose...   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  get takeout last night horrible something must...  \n",
       "1  girl sweet price reasonable stand bed hot make...  \n",
       "2  rudest people eveencountered husband wife own ...  \n",
       "3  airport covet fothe destination lead flier vie...  \n",
       "4  last month show steady decline pisspooservice ...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test_feature = count_vectorizer_top_features.transform(result_test_df['cleaned_review'])\n",
    "result_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e69fdba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 17909)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_feature_array = result_test_feature.toarray()\n",
    "result_feature_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc8d4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_feature_agg(selected_feature_array):\n",
    "    weighted_feat_array = []\n",
    "    for i in range(0, len(selected_feature_array)):        \n",
    "            weighted_feat_array.append(np.multiply(selected_feature_array[i],mx_weight))        \n",
    "    return weighted_feat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d02410bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_feature_weighted = weighted_feature_agg(result_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "056e40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifer.predict(result_feature_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c5d4a48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d61f442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "outfile = open('results.csv','w')\n",
    "out = csv.writer(outfile)\n",
    "out.writerows(map(lambda x: [x], prediction))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d1939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
