{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1498,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1499,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:29:28.319045Z",
     "iopub.status.busy": "2022-09-07T20:29:28.318572Z",
     "iopub.status.idle": "2022-09-07T20:29:28.603267Z",
     "shell.execute_reply": "2022-09-07T20:29:28.602081Z",
     "shell.execute_reply.started": "2022-09-07T20:29:28.319011Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('1661892619_92027_train_file.csv',names=[\"sentiments\", \"reviews\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text cleaning, tokenization and stopwords filtering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:29:29.703652Z",
     "iopub.status.busy": "2022-09-07T20:29:29.703190Z",
     "iopub.status.idle": "2022-09-07T20:29:30.930710Z",
     "shell.execute_reply": "2022-09-07T20:29:30.929762Z",
     "shell.execute_reply.started": "2022-09-07T20:29:29.703595Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def basic_cleaning(data_frame):\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.lower())\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.translate(str.maketrans('', '', string.punctuation)))\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.translate(str.maketrans('', '', string.digits)))\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: re.sub(\"r[^a-z]\",'',review))\n",
    "\n",
    "    \n",
    "import nltk\n",
    "\n",
    "def tokenize_data(data_frame):\n",
    "    data_frame['words'] = data_frame.reviews.apply(lambda review: nltk.word_tokenize(review))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(list):\n",
    "    stop_words_removed = []\n",
    "    for i in list:\n",
    "        if i not in stopwords:\n",
    "            stop_words_removed.append(i)\n",
    "    return stop_words_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tagging Part of Speech and extraction of root words from tokenized words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:29:39.969816Z",
     "iopub.status.busy": "2022-09-07T20:29:39.969420Z",
     "iopub.status.idle": "2022-09-07T20:30:27.157526Z",
     "shell.execute_reply": "2022-09-07T20:30:27.155569Z",
     "shell.execute_reply.started": "2022-09-07T20:29:39.969778Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('all') {Depending on the situation, might want to UNCOMMENT this to use nltk pos_tag}\n",
    "#tag part of speech to get more accurate word during lemmatizaton\n",
    "def tag_pos(list_of_words):\n",
    "    return nltk.pos_tag(list_of_words)\n",
    "\n",
    "#extraction of lemma words after pos taggin \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "def find_lemma_word(word):\n",
    "    lemma_words=[]\n",
    "    words_with_pos = tag_pos(word)\n",
    "    for word in words_with_pos:\n",
    "        if word[1].startswith('NN'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='n'))\n",
    "        elif word[1].startswith('VB'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='v'))\n",
    "        elif word[1].startswith('JJ'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='a'))\n",
    "        elif word[1].startswith('RB'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='r'))\n",
    "        else:\n",
    "            lemma_words.append(word[0])\n",
    "            \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:30:27.161877Z",
     "iopub.status.busy": "2022-09-07T20:30:27.161403Z",
     "iopub.status.idle": "2022-09-07T20:32:05.436075Z",
     "shell.execute_reply": "2022-09-07T20:32:05.434776Z",
     "shell.execute_reply.started": "2022-09-07T20:30:27.161838Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "basic_cleaning(train_data)\n",
    "tokenize_data(train_data)\n",
    "train_data['stop_words_cleaned'] = train_data.words.apply(lambda word_list: remove_stopwords(word_list))\n",
    "train_data['lemma_word'] = train_data.stop_words_cleaned.apply(lambda word_list: find_lemma_word(word_list))\n",
    "train_data['cleaned_review'] = train_data.lemma_word.apply(lambda review_list: \" \".join(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:05.439219Z",
     "iopub.status.busy": "2022-09-07T20:32:05.438473Z",
     "iopub.status.idle": "2022-09-07T20:32:05.495295Z",
     "shell.execute_reply": "2022-09-07T20:32:05.494068Z",
     "shell.execute_reply.started": "2022-09-07T20:32:05.439178Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       sentiments                                            reviews  \\\n0              -1  eat at fioris they said  youll like it they sa...   \n1              -1  i just dont understand the appeal  ive tried t...   \n2               1  this is my go to place foa really good beef en...   \n3              -1  not impressed when i ordered the oyako bowl th...   \n4              -1  this is the first time evei wrote a bad review...   \n...           ...                                                ...   \n17995          -1  i was referred to go to this place by a buddy ...   \n17996           1  the food here was really good  we started off ...   \n17997           1  i eat at this place maybe  a week i am die har...   \n17998           1  phoenix airport is getting betteday by day  i ...   \n17999          -1  so when i was much youngei went to christos an...   \n\n                                                   words  \\\n0      [eat, at, fioris, they, said, youll, like, it,...   \n1      [i, just, dont, understand, the, appeal, ive, ...   \n2      [this, is, my, go, to, place, foa, really, goo...   \n3      [not, impressed, when, i, ordered, the, oyako,...   \n4      [this, is, the, first, time, evei, wrote, a, b...   \n...                                                  ...   \n17995  [i, was, referred, to, go, to, this, place, by...   \n17996  [the, food, here, was, really, good, we, start...   \n17997  [i, eat, at, this, place, maybe, a, week, i, a...   \n17998  [phoenix, airport, is, getting, betteday, by, ...   \n17999  [so, when, i, was, much, youngei, went, to, ch...   \n\n                                      stop_words_cleaned  \\\n0      [eat, fioris, said, youll, like, saidnnis, con...   \n1      [dont, understand, appeal, ive, tried, place, ...   \n2      [go, place, foa, really, good, beef, enchilada...   \n3      [impressed, ordered, oyako, bowl, conversation...   \n4      [first, time, evei, wrote, bad, review, frustr...   \n...                                                  ...   \n17995  [referred, go, place, buddy, aftea, conversati...   \n17996  [food, really, good, started, garlic, bread, c...   \n17997  [eat, place, maybe, week, die, hard, wing, fan...   \n17998  [phoenix, airport, getting, betteday, day, pri...   \n17999  [much, youngei, went, christos, first, place, ...   \n\n                                              lemma_word  \\\n0      [eat, fioris, say, youll, like, saidnnis, conv...   \n1      [dont, understand, appeal, ive, tried, place, ...   \n2      [go, place, foa, really, good, beef, enchilada...   \n3      [impressed, order, oyako, bowl, conversation, ...   \n4      [first, time, evei, write, bad, review, frustr...   \n...                                                  ...   \n17995  [refer, go, place, buddy, aftea, conversation,...   \n17996  [food, really, good, start, garlic, bread, cov...   \n17997  [eat, place, maybe, week, die, hard, wing, fan...   \n17998  [phoenix, airport, get, betteday, day, primari...   \n17999  [much, youngei, go, christos, first, place, es...   \n\n                                          cleaned_review  \n0      eat fioris say youll like saidnnis convenientl...  \n1      dont understand appeal ive tried place twice t...  \n2      go place foa really good beef enchilada red sa...  \n3      impressed order oyako bowl conversation go som...  \n4      first time evei write bad review frustrate her...  \n...                                                  ...  \n17995  refer go place buddy aftea conversation get sh...  \n17996  food really good start garlic bread cover toma...  \n17997  eat place maybe week die hard wing fan best ev...  \n17998  phoenix airport get betteday day primarily use...  \n17999  much youngei go christos first place escargot ...  \n\n[18000 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiments</th>\n      <th>reviews</th>\n      <th>words</th>\n      <th>stop_words_cleaned</th>\n      <th>lemma_word</th>\n      <th>cleaned_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>eat at fioris they said  youll like it they sa...</td>\n      <td>[eat, at, fioris, they, said, youll, like, it,...</td>\n      <td>[eat, fioris, said, youll, like, saidnnis, con...</td>\n      <td>[eat, fioris, say, youll, like, saidnnis, conv...</td>\n      <td>eat fioris say youll like saidnnis convenientl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1</td>\n      <td>i just dont understand the appeal  ive tried t...</td>\n      <td>[i, just, dont, understand, the, appeal, ive, ...</td>\n      <td>[dont, understand, appeal, ive, tried, place, ...</td>\n      <td>[dont, understand, appeal, ive, tried, place, ...</td>\n      <td>dont understand appeal ive tried place twice t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>this is my go to place foa really good beef en...</td>\n      <td>[this, is, my, go, to, place, foa, really, goo...</td>\n      <td>[go, place, foa, really, good, beef, enchilada...</td>\n      <td>[go, place, foa, really, good, beef, enchilada...</td>\n      <td>go place foa really good beef enchilada red sa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1</td>\n      <td>not impressed when i ordered the oyako bowl th...</td>\n      <td>[not, impressed, when, i, ordered, the, oyako,...</td>\n      <td>[impressed, ordered, oyako, bowl, conversation...</td>\n      <td>[impressed, order, oyako, bowl, conversation, ...</td>\n      <td>impressed order oyako bowl conversation go som...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1</td>\n      <td>this is the first time evei wrote a bad review...</td>\n      <td>[this, is, the, first, time, evei, wrote, a, b...</td>\n      <td>[first, time, evei, wrote, bad, review, frustr...</td>\n      <td>[first, time, evei, write, bad, review, frustr...</td>\n      <td>first time evei write bad review frustrate her...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17995</th>\n      <td>-1</td>\n      <td>i was referred to go to this place by a buddy ...</td>\n      <td>[i, was, referred, to, go, to, this, place, by...</td>\n      <td>[referred, go, place, buddy, aftea, conversati...</td>\n      <td>[refer, go, place, buddy, aftea, conversation,...</td>\n      <td>refer go place buddy aftea conversation get sh...</td>\n    </tr>\n    <tr>\n      <th>17996</th>\n      <td>1</td>\n      <td>the food here was really good  we started off ...</td>\n      <td>[the, food, here, was, really, good, we, start...</td>\n      <td>[food, really, good, started, garlic, bread, c...</td>\n      <td>[food, really, good, start, garlic, bread, cov...</td>\n      <td>food really good start garlic bread cover toma...</td>\n    </tr>\n    <tr>\n      <th>17997</th>\n      <td>1</td>\n      <td>i eat at this place maybe  a week i am die har...</td>\n      <td>[i, eat, at, this, place, maybe, a, week, i, a...</td>\n      <td>[eat, place, maybe, week, die, hard, wing, fan...</td>\n      <td>[eat, place, maybe, week, die, hard, wing, fan...</td>\n      <td>eat place maybe week die hard wing fan best ev...</td>\n    </tr>\n    <tr>\n      <th>17998</th>\n      <td>1</td>\n      <td>phoenix airport is getting betteday by day  i ...</td>\n      <td>[phoenix, airport, is, getting, betteday, by, ...</td>\n      <td>[phoenix, airport, getting, betteday, day, pri...</td>\n      <td>[phoenix, airport, get, betteday, day, primari...</td>\n      <td>phoenix airport get betteday day primarily use...</td>\n    </tr>\n    <tr>\n      <th>17999</th>\n      <td>-1</td>\n      <td>so when i was much youngei went to christos an...</td>\n      <td>[so, when, i, was, much, youngei, went, to, ch...</td>\n      <td>[much, youngei, went, christos, first, place, ...</td>\n      <td>[much, youngei, go, christos, first, place, es...</td>\n      <td>much youngei go christos first place escargot ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>18000 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 1503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Test Split and volabulary extraction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:11.159062Z",
     "iopub.status.busy": "2022-09-07T20:32:11.158649Z",
     "iopub.status.idle": "2022-09-07T20:32:11.178064Z",
     "shell.execute_reply": "2022-09-07T20:32:11.176957Z",
     "shell.execute_reply.started": "2022-09-07T20:32:11.159019Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_review, test_review, train_sentiment, test_sentiment =train_test_split(train_data.cleaned_review,train_data.sentiments,shuffle=True,random_state=0,stratify=train_data.sentiments,train_size=.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:11.924779Z",
     "iopub.status.busy": "2022-09-07T20:32:11.924343Z",
     "iopub.status.idle": "2022-09-07T20:32:17.315920Z",
     "shell.execute_reply": "2022-09-07T20:32:17.314576Z",
     "shell.execute_reply.started": "2022-09-07T20:32:11.924743Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(\n",
    "    min_df=.0095,  # only include words occuring in .95% of the document\n",
    "    use_idf=False, # We don't need inerse doc frequency as we are inerested in only vocabulary extraction\n",
    "    ngram_range=(1,2)) # this will consider only one and two words (combined) as features\n",
    "\n",
    "feature_matrix = tf_vectorizer.fit_transform(train_review)\n",
    "feature_array = feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:17.319084Z",
     "iopub.status.busy": "2022-09-07T20:32:17.318121Z",
     "iopub.status.idle": "2022-09-07T20:32:17.418220Z",
     "shell.execute_reply": "2022-09-07T20:32:17.416994Z",
     "shell.execute_reply.started": "2022-09-07T20:32:17.319042Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['able', 'absolutely', 'accommodate', 'across', 'act', 'actually',\n       'add', 'admit', 'afte', 'aftea', 'aftei', 'afternoon', 'aftethe',\n       'ago', 'agree', 'ahead', 'airport', 'allow', 'almost', 'alone',\n       'along', 'already', 'also', 'although', 'always', 'amaze',\n       'amazing', 'ambiance', 'american', 'amount', 'annoy', 'anothe',\n       'anyone', 'anything', 'anyway', 'anywhere', 'apologize',\n       'apparently', 'appointment', 'appreciate', 'area', 'arent',\n       'arizona', 'around', 'arrive', 'ask', 'ask foa', 'ate',\n       'atmosphere', 'attention', 'attentive', 'attitude', 'authentic',\n       'available', 'average', 'avoid', 'away', 'awesome', 'awful', 'az',\n       'baand', 'back', 'bacon', 'bad', 'bag', 'bar', 'barely', 'base',\n       'basic', 'basically', 'bathroom', 'bbq', 'bean', 'beat',\n       'beautiful', 'become', 'bed', 'beef', 'beer', 'begin', 'behind',\n       'believe', 'best', 'bettethan', 'beyond', 'big', 'bill',\n       'birthday', 'bit', 'bite', 'black', 'bland', 'block', 'blue',\n       'book', 'bottle', 'bottom', 'bowl', 'box', 'boy', 'boyfriend',\n       'bread', 'break', 'breakfast', 'bring', 'brown', 'buck', 'buffet',\n       'build', 'building', 'bunch', 'burger', 'burrito', 'business',\n       'busy', 'buy', 'cafe', 'cake', 'call', 'cant', 'car', 'card',\n       'care', 'carry', 'case', 'cash', 'catch', 'cause', 'central',\n       'certainly', 'chain', 'chance', 'change', 'charge', 'charlotte',\n       'cheap', 'check', 'cheese', 'chef', 'chicken', 'child', 'chili',\n       'chinese', 'chinese food', 'chip', 'chocolate', 'choice', 'choose',\n       'city', 'class', 'clean', 'clearly', 'close', 'club', 'coffee',\n       'cold', 'combo', 'come', 'come back', 'comfortable', 'comment',\n       'company', 'compare', 'complain', 'complaint', 'complete',\n       'completely', 'consider', 'contact', 'continue', 'conversation',\n       'cook', 'cool', 'corn', 'cost', 'could', 'could get', 'couldnt',\n       'country', 'couple', 'coupon', 'course', 'cover', 'crab', 'crap',\n       'crave', 'crazy', 'cream', 'credit', 'crispy', 'crowd', 'crust',\n       'cup', 'customer', 'customeservice', 'cut', 'cute', 'damn', 'dark',\n       'date', 'day', 'deal', 'decent', 'decide', 'definitely',\n       'delicious', 'deliver', 'delivery', 'desert', 'desk', 'despite',\n       'dessert', 'didnt', 'didnt even', 'didnt want', 'die', 'different',\n       'difficult', 'din', 'dip', 'dirty', 'disappoint', 'disappointed',\n       'discount', 'disgust', 'dish', 'do', 'doesnt', 'dog', 'dollar',\n       'dont', 'dont get', 'dont know', 'dont think', 'doubt', 'downtown',\n       'dress', 'drink', 'drive', 'drop', 'dry', 'due', 'early', 'easily',\n       'east', 'easy', 'eat', 'eaten', 'egg', 'else', 'elsewhere',\n       'employee', 'empty', 'end', 'enjoy', 'enough', 'entire', 'entree',\n       'especially', 'establishment', 'etc', 'evehad', 'even',\n       'even though', 'event', 'every', 'every time', 'everyone',\n       'everything', 'exactly', 'excellent', 'except', 'excite', 'expect',\n       'expectation', 'expensive', 'experience', 'explain', 'extra',\n       'extremely', 'eye', 'face', 'fact', 'fairly', 'fall', 'family',\n       'fan', 'fantastic', 'fast', 'favorite', 'fee', 'feel', 'feel like',\n       'felt', 'felt like', 'figure', 'fill', 'finally', 'find', 'fine',\n       'finish', 'first', 'first time', 'fish', 'fit', 'five', 'fix',\n       'flavor', 'flavorful', 'fly', 'fo', 'foa', 'foabout', 'foan',\n       'foit', 'folk', 'follow', 'folunch', 'fome', 'fomy', 'food',\n       'food good', 'food great', 'food service', 'foot', 'forget',\n       'forward', 'fosome', 'fothat', 'fothe', 'fothe first', 'fothem',\n       'fothis', 'fotwo', 'fous', 'fowhat', 'foyears', 'foyou', 'free',\n       'french', 'frequent', 'fresh', 'friday', 'fried', 'friend',\n       'friendly', 'front', 'front desk', 'frozen', 'fry', 'full', 'fun',\n       'game', 'garden', 'garlic', 'general', 'generally', 'get', 'gift',\n       'girl', 'give', 'give place', 'give us', 'glad', 'glass', 'go',\n       'go back', 'go get', 'god', 'good', 'good food', 'good service',\n       'good thing', 'grab', 'greasy', 'great', 'great food',\n       'great place', 'great service', 'green', 'greet', 'grill',\n       'grocery', 'gross', 'ground', 'group', 'grow', 'guess', 'guest',\n       'guy', 'hadnt', 'half', 'hand', 'handle', 'hang', 'happen',\n       'happy', 'hard', 'hate', 'havent', 'head', 'heard', 'heat', 'hell',\n       'help', 'helpful', 'high', 'highly', 'highly recommend', 'hit',\n       'hold', 'hole', 'home', 'honest', 'honestly', 'hop', 'hope',\n       'horrible', 'hostess', 'hot', 'hotel', 'hour', 'house',\n       'howevethe', 'huge', 'hungry', 'husband', 'ice', 'ice cream', 'id',\n       'idea', 'ignore', 'ill', 'im', 'im sure', 'imagine', 'immediately',\n       'impress', 'impressed', 'include', 'incredibly', 'inform',\n       'ingredient', 'inside', 'instead', 'interest', 'isnt', 'issue',\n       'italian', 'item', 'ive', 'ive evehad', 'job', 'joint', 'joke',\n       'keep', 'kid', 'kind', 'kinda', 'kitchen', 'know', 'knowledgeable',\n       'la', 'lack', 'lady', 'large', 'last', 'last night', 'last time',\n       'late', 'learn', 'least', 'leave', 'less', 'let', 'lettuce',\n       'level', 'life', 'light', 'like', 'like place', 'line', 'list',\n       'listen', 'literally', 'little', 'live', 'local', 'locate',\n       'location', 'long', 'long time', 'look', 'look foa',\n       'look forward', 'look like', 'lose', 'lot', 'loud', 'love',\n       'love place', 'low', 'lunch', 'main', 'make', 'make feel',\n       'make sure', 'mall', 'man', 'manage', 'management', 'many',\n       'market', 'may', 'maybe', 'meal', 'mean', 'meat', 'mediocre',\n       'medium', 'meet', 'mention', 'menu', 'mess', 'mexican',\n       'mexican food', 'middle', 'might', 'mile', 'min', 'mind', 'mine',\n       'minute', 'miss', 'mistake', 'mix', 'mixed', 'mom', 'moment',\n       'money', 'month', 'morning', 'mostly', 'move', 'much', 'mushroom',\n       'music', 'must', 'name', 'nearly', 'need', 'negative',\n       'neighborhood', 'nevebeen', 'nevego', 'nevehad', 'new', 'next',\n       'next time', 'ni', 'nice', 'night', 'nni', 'nnmy', 'nnthe',\n       'nnthis', 'nnwe', 'none', 'noodle', 'normally', 'note', 'nothing',\n       'notice', 'nthe', 'oa', 'obviously', 'occasion', 'offer', 'office',\n       'often', 'oh', 'oil', 'ok', 'okay', 'old', 'olive', 'one',\n       'one best', 'onion', 'online', 'open', 'opinion', 'option',\n       'order', 'ordered', 'ordethe', 'othe', 'others', 'otherwise',\n       'othethan', 'oufood', 'outable', 'outside', 'ouwaitress', 'ove',\n       'overall', 'overprice', 'ovethe', 'oveto', 'owner', 'pack', 'park',\n       'parking', 'part', 'party', 'pas', 'pass', 'past', 'pasta',\n       'patio', 'pay', 'people', 'perfect', 'perfectly', 'perhaps',\n       'person', 'phoenix', 'phone', 'pick', 'picture', 'piece',\n       'pittsburgh', 'pizza', 'place', 'place get', 'place go', 'plain',\n       'plan', 'plate', 'play', 'pleasant', 'please', 'plenty', 'plus',\n       'pm', 'point', 'pool', 'pop', 'pork', 'portion', 'positive',\n       'possible', 'post', 'potato', 'prepare', 'pretty', 'pretty good',\n       'pretty much', 'price', 'pricey', 'probably', 'problem', 'process',\n       'product', 'professional', 'provide', 'pull', 'purchase', 'put',\n       'quality', 'question', 'quick', 'quickly', 'quite', 'rare', 'rate',\n       'read', 'ready', 'real', 'realize', 'really', 'really good',\n       'really like', 'reason', 'reasonable', 'receive', 'recently',\n       'recommend', 'recommend place', 'red', 'refill', 'refuse',\n       'remind', 'replace', 'request', 'reservation', 'rest',\n       'restaurant', 'return', 'review', 'rib', 'rice', 'ridiculous',\n       'right', 'ring', 'road', 'rock', 'roll', 'room', 'rude', 'run',\n       'rush', 'sad', 'salad', 'sale', 'salmon', 'salon', 'salsa', 'salt',\n       'salty', 'sandwich', 'sat', 'saturday', 'sauce', 'sausage', 'save',\n       'saw', 'say', 'say would', 'schedule', 'school', 'scottsdale',\n       'seafood', 'search', 'season', 'seat', 'second', 'section', 'see',\n       'seem', 'seem like', 'selection', 'sell', 'send', 'sense',\n       'seriously', 'serve', 'server', 'service', 'service good',\n       'service great', 'set', 'several', 'several time', 'share', 'shop',\n       'short', 'shot', 'show', 'shrimp', 'sick', 'side', 'sign',\n       'simple', 'simply', 'since', 'single', 'sit', 'situation', 'size',\n       'slice', 'slightly', 'slow', 'small', 'smell', 'smile', 'smoke',\n       'soda', 'soft', 'soggy', 'someone', 'something', 'sometimes',\n       'somewhere', 'son', 'soon', 'sorry', 'sort', 'sound', 'soup',\n       'space', 'speak', 'special', 'spend', 'spice', 'spicy', 'sport',\n       'spot', 'st', 'staff', 'staff friendly', 'stand', 'standard',\n       'star', 'start', 'state', 'stay', 'steak', 'step', 'stick',\n       'still', 'stop', 'store', 'straight', 'street', 'strip', 'stuff',\n       'style', 'suck', 'suggest', 'sunday', 'suppose', 'sure',\n       'surprise', 'sushi', 'sweet', 'table', 'taco', 'take', 'take care',\n       'talk', 'taste', 'taste like', 'tasty', 'tea', 'tell', 'tell us',\n       'terrible', 'th', 'thank', 'thanks', 'thats', 'theifood', 'there',\n       'theyre', 'theyve', 'thick', 'thin', 'thing', 'think', 'though',\n       'thought', 'three', 'throw', 'time', 'time go', 'tiny', 'tip',\n       'tire', 'toast', 'today', 'told', 'tomato', 'ton', 'tonight',\n       'top', 'tortilla', 'total', 'totally', 'touch', 'town', 'travel',\n       'treat', 'tried', 'trip', 'true', 'truly', 'trust', 'try', 'turn',\n       'tv', 'twice', 'two', 'type', 'typical', 'understand',\n       'unfortunately', 'unique', 'unless', 'update', 'upon', 'us', 'use',\n       'usually', 'valley', 'value', 'variety', 'vegetable', 'vegetarian',\n       'veggie', 'view', 'visit', 'wait', 'wait staff', 'waitress',\n       'walk', 'wall', 'want', 'wanted', 'warm', 'wash', 'wasnt', 'waste',\n       'watch', 'way', 'wed', 'week', 'weekend', 'weird', 'welcome',\n       'well', 'werent', 'weve', 'whats', 'white', 'whole', 'wife',\n       'window', 'wine', 'wing', 'wish', 'within', 'without', 'woman',\n       'wonderful', 'wont', 'word', 'work', 'world', 'worth', 'would',\n       'would go', 'would recommend', 'wouldnt', 'wow', 'wrap', 'write',\n       'wrong', 'yeah', 'year', 'year ago', 'yelp', 'yes', 'yesterday',\n       'yet', 'youd', 'youll', 'young', 'youre', 'yummy', 'zero'],\n      dtype=object)"
     },
     "execution_count": 1506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = tf_vectorizer.get_feature_names_out()\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select K-best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "outputs": [
    {
     "data": {
      "text/plain": "array([117.49553041,  72.7897542 ,  29.98804258,  56.44754701,\n        74.36370902,  43.38379699,  16.55056932, 152.39033186,\n        17.66473916,  92.98613464,  52.13520338,  41.84771814,\n        24.46388616,  35.2801613 ,  25.49318514,  22.32829091,\n        33.07515707,  99.77394068,  49.65302297,  34.58289896,\n        17.93845493,  22.27007106,  23.60806556,  26.7673348 ,\n        19.04477528,  24.42742745,  63.4631454 ,  20.14418664,\n        43.48158212,  67.12754306,  17.01099046,  18.24795903,\n        38.0899651 ,  92.78385777,  28.89877203,  27.24625673,\n        37.52213123, 338.49366885,  25.0368445 ,  35.85577198,\n        26.1011196 ,  24.15164985,  16.57736668,  26.48603526,\n        33.07236701,  25.66369063,  37.90444654,  65.99557239,\n        19.73994145,  17.31681769,  22.40461393,  43.53327206,\n        22.20017458, 199.0904724 ,  39.46791812,  17.97674507,\n        41.02583686,  69.46757439,  23.09513494,  42.86334196,\n        55.14142096,  26.32131335,  19.40438743,  36.01610419,\n        35.96734404,  47.44767753,  50.27921184,  67.65355924,\n        34.72586186,  43.48656141,  67.59245794,  49.40965466,\n        24.9922795 ,  16.85191083,  27.54201362,  16.36465299,\n        17.79420871,  17.78574066,  20.62273366,  19.23410585,\n        35.24142913,  24.24159866,  16.39759203,  73.8359585 ,\n        64.6045142 ,  16.76957905,  20.91306733,  34.71867623,\n        32.7396128 ,  19.46378758,  36.24744223,  40.78116109,\n        26.83664416,  17.38463427,  39.26466403,  20.20153346,\n        49.61269344,  16.32765959,  26.5588186 ])"
     },
     "execution_count": 1507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "vocab_list = tf_vectorizer.get_feature_names_out() # extract vocabulary from the prevous step\n",
    "select_k_best = SelectKBest(score_func=chi2, k= int(len(vocab_list)*.104)) # Only selecting top 10.4% of vocabulary based on the chi-square values\n",
    "train_sentiment_np_array = np.array(train_sentiment)\n",
    "select_k_best.fit(feature_array, train_sentiment_np_array)\n",
    "mask = select_k_best.get_support()\n",
    "k_best_feature = vocab_list[mask] # Select only K-best vocabulary as features for training\n",
    "k_score = select_k_best.scores_\n",
    "k_score = k_score[mask]\n",
    "k_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "outputs": [
    {
     "data": {
      "text/plain": "(99,)"
     },
     "execution_count": 1508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_best_feature.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Supervised Chi-Square weight for k-best term by utilising target value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "outputs": [],
   "source": [
    "# Build Tf-Idf table with K-best features selected in the previous step\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "selected_tf_vectorizer = TfidfVectorizer(use_idf=True, vocabulary=k_best_feature, sublinear_tf=True, ngram_range=(1,2))\n",
    "selected_feat_array = selected_tf_vectorizer.fit_transform(train_review).toarray()\n",
    "selected_test_array = selected_tf_vectorizer.transform(test_review).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "outputs": [],
   "source": [
    "# To extract Chi-sqare value we need observed and expected frequency table\n",
    "observed_value_table = pd.DataFrame(selected_tf_vectorizer.get_feature_names_out(), columns=['features'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "outputs": [],
   "source": [
    "# Loading Tf-Idf and sentiments to Data Frame for calculating chi-sqare value\n",
    "tf_data_frame = pd.DataFrame(selected_feat_array,columns = selected_tf_vectorizer.get_feature_names_out())\n",
    "tf_data_frame['sentiments'] = np.array(train_sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "outputs": [
    {
     "data": {
      "text/plain": "     features  positive_sentiment  negative_sentiment  total_row_count\n0      always          603.189705          224.982624       828.172328\n1       amaze          266.453306           37.288465       303.741771\n2     amazing          136.244773           24.295907       160.540680\n3         ask          180.898781          441.135587       622.034369\n4     awesome          310.024743           61.789369       371.814112\n..        ...                 ...                 ...              ...\n94  wonderful          172.932561           34.279144       207.211704\n95       wont           86.824651          219.829920       306.654571\n96      would          382.290551          665.825364      1048.115914\n97    wouldnt           50.765720          150.047241       200.812961\n98      yummy          115.292834           17.980619       133.273454\n\n[99 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>positive_sentiment</th>\n      <th>negative_sentiment</th>\n      <th>total_row_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>always</td>\n      <td>603.189705</td>\n      <td>224.982624</td>\n      <td>828.172328</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>amaze</td>\n      <td>266.453306</td>\n      <td>37.288465</td>\n      <td>303.741771</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>amazing</td>\n      <td>136.244773</td>\n      <td>24.295907</td>\n      <td>160.540680</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ask</td>\n      <td>180.898781</td>\n      <td>441.135587</td>\n      <td>622.034369</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>awesome</td>\n      <td>310.024743</td>\n      <td>61.789369</td>\n      <td>371.814112</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>wonderful</td>\n      <td>172.932561</td>\n      <td>34.279144</td>\n      <td>207.211704</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>wont</td>\n      <td>86.824651</td>\n      <td>219.829920</td>\n      <td>306.654571</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>would</td>\n      <td>382.290551</td>\n      <td>665.825364</td>\n      <td>1048.115914</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>wouldnt</td>\n      <td>50.765720</td>\n      <td>150.047241</td>\n      <td>200.812961</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>yummy</td>\n      <td>115.292834</td>\n      <td>17.980619</td>\n      <td>133.273454</td>\n    </tr>\n  </tbody>\n</table>\n<p>99 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 1512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the sum of features presence according to its presence in positive and negative reviews\n",
    "observed_value_table['positive_sentiment'] = observed_value_table.features.apply(lambda feature: tf_data_frame.loc[tf_data_frame['sentiments']==1,feature].sum())\n",
    "observed_value_table['negative_sentiment'] = observed_value_table.features.apply(lambda feature: tf_data_frame.loc[tf_data_frame['sentiments']==-1,feature].sum())\n",
    "observed_value_table['total_row_count'] = observed_value_table['positive_sentiment'] + observed_value_table['negative_sentiment']\n",
    "observed_value_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "outputs": [],
   "source": [
    "# Calculated expected value table\n",
    "chi_sqare_expected = pd.DataFrame()\n",
    "\n",
    "def chi_sqare_value(row_totals, positive_column_total, negative_column_total, total_sum):\n",
    "    positive_chi_value = []\n",
    "    negative_chi_value = []\n",
    "    for i in range(0, len(row_totals)):\n",
    "        positive_chi_value.append((row_totals[i]*positive_column_total)/total_sum)\n",
    "        negative_chi_value.append((row_totals[i]*negative_column_total)/total_sum)\n",
    "    return positive_chi_value, negative_chi_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "outputs": [],
   "source": [
    "row_totals = np.array(observed_value_table['total_row_count'])\n",
    "positive_column_total = observed_value_table['positive_sentiment'].sum()\n",
    "negative_column_total = observed_value_table['negative_sentiment'].sum()\n",
    "total_sum = observed_value_table['total_row_count'].sum()\n",
    "expected_positive, expected_negative = chi_sqare_value(row_totals, positive_column_total, negative_column_total, total_sum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "outputs": [],
   "source": [
    "chi_sqare_expected['+1'] = np.array(expected_positive)\n",
    "chi_sqare_expected['-1'] = np.array(expected_negative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "outputs": [],
   "source": [
    "# This will be our FINAL Chi-Square table derived from observed and expected Frequency\n",
    "chi_sqare_table = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "outputs": [],
   "source": [
    "chi_sqare_table['positiev_sentiments'] = ((observed_value_table['positive_sentiment'] - chi_sqare_expected['+1'])**2)/chi_sqare_expected['+1']\n",
    "chi_sqare_table['negatiev_sentiments'] = ((observed_value_table['negative_sentiment'] - chi_sqare_expected['-1'])**2)/chi_sqare_expected['-1']\n",
    "chi_sqare_table['sum'] = chi_sqare_table['positiev_sentiments'] + chi_sqare_table['negatiev_sentiments']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "outputs": [
    {
     "data": {
      "text/plain": "    positiev_sentiments  negatiev_sentiments         sum\n0            102.827931            96.203665  199.031596\n1             97.489390            91.209038  188.698429\n2             44.322325            41.467042   85.789366\n3             47.707891            44.634507   92.342397\n4             94.477328            88.391015  182.868343\n..                  ...                  ...         ...\n94            52.878628            49.472140  102.350768\n95            25.433782            23.795315   49.229097\n96            30.509930            28.544454   59.054384\n97            22.084196            20.661512   42.745708\n98            40.177005            37.588767   77.765771\n\n[99 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>positiev_sentiments</th>\n      <th>negatiev_sentiments</th>\n      <th>sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>102.827931</td>\n      <td>96.203665</td>\n      <td>199.031596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>97.489390</td>\n      <td>91.209038</td>\n      <td>188.698429</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>44.322325</td>\n      <td>41.467042</td>\n      <td>85.789366</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>47.707891</td>\n      <td>44.634507</td>\n      <td>92.342397</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>94.477328</td>\n      <td>88.391015</td>\n      <td>182.868343</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>52.878628</td>\n      <td>49.472140</td>\n      <td>102.350768</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>25.433782</td>\n      <td>23.795315</td>\n      <td>49.229097</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>30.509930</td>\n      <td>28.544454</td>\n      <td>59.054384</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>22.084196</td>\n      <td>20.661512</td>\n      <td>42.745708</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>40.177005</td>\n      <td>37.588767</td>\n      <td>77.765771</td>\n    </tr>\n  </tbody>\n</table>\n<p>99 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 1518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_sqare_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create weighted features by utilising respective chi-square weight of features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "outputs": [],
   "source": [
    "def generate_chi_square_weighted_features(feat_to_be_weighted, chi_weight):\n",
    "    new_weighted_feature= []\n",
    "    numpy_feat_array = np.array(feat_to_be_weighted)\n",
    "    for i in range(0,len(numpy_feat_array)):\n",
    "            new_weighted_feature.append(np.multiply(numpy_feat_array[i], chi_weight))\n",
    "    return new_weighted_feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "outputs": [],
   "source": [
    "chi_weight = np.array (chi_sqare_table['sum'])\n",
    "weighted_selected_feat = np.array(generate_chi_square_weighted_features(selected_feat_array, chi_weight))\n",
    "weighted_test_feat = np.array(generate_chi_square_weighted_features( selected_test_array, chi_weight))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-NN from scratch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distance of a test data from training data\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def calculate_neighbour(train_data, test_data):\n",
    "    \n",
    "    numpy_distances = np.array(euclidean_distances(train_data,[test_data]).flatten())\n",
    "    inv_distance = []\n",
    "\n",
    "    # Divided by zero exception handeled by adding 1.0000000000000004e+63 (This is weight of a test data point which is exactly same as train point)\n",
    "    for i in numpy_distances:\n",
    "        inv_distance.append(np.divide(1,max(i**3,.000000000000000000001**3))) # taking inverse of distance because nearest neighbours will have higher contribution in prediction\n",
    "\n",
    "    inv_distance = np.array(inv_distance)\n",
    "    indexes_by_shortest_dist = inv_distance.argsort() # This returns the indexes after shoring the distances\n",
    "    # Reverse it as Higher Weight corresponds to nearest neighbour\n",
    "    return  np.flip(indexes_by_shortest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_majority_and_predict(train_data, test_data, train_sentiments, K):\n",
    "    nearest_neighbors_indexes = calculate_neighbour(train_data, test_data)\n",
    "    sentiment_classes = []\n",
    "    # take sentiments of K-best train data points which best represents the class of test data point\n",
    "    for i in range(0,K):\n",
    "        sentiment_classes.append(train_sentiments[nearest_neighbors_indexes[i]])\n",
    "    return max(sentiment_classes, key=sentiment_classes.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment\n",
    "train_sentiments_array = np.array(train_sentiment)\n",
    "test_sentiments_array = np.array(test_sentiment)\n",
    "predicted_sentiments = []\n",
    "print(len(weighted_test_feat))\n",
    "for i in range(0,len(weighted_test_feat)):\n",
    "    # print(i)\n",
    "    predicted_sentiments.append(find_majority_and_predict(weighted_selected_feat,weighted_test_feat[i], train_sentiments_array, 23))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-07T21:57:54.970066Z",
     "iopub.status.idle": "2022-09-07T21:57:54.971113Z",
     "shell.execute_reply": "2022-09-07T21:57:54.970892Z",
     "shell.execute_reply.started": "2022-09-07T21:57:54.970867Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Calulate accuracy of prediction by cross validation\n",
    "def find_accuracy(predicted_sentiment, real_sentiment):\n",
    "    correct_prediction = 0\n",
    "    for i in range(0,len(predicted_sentiments)):\n",
    "        if predicted_sentiment[i] == real_sentiment[i]:\n",
    "            correct_prediction += 1\n",
    "    return np.divide(correct_prediction, len(predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-07T21:57:54.972155Z",
     "iopub.status.idle": "2022-09-07T21:57:54.972788Z",
     "shell.execute_reply": "2022-09-07T21:57:54.972584Z",
     "shell.execute_reply.started": "2022-09-07T21:57:54.972562Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(find_accuracy(predicted_sentiments,test_sentiments_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare Unlabeled Test Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:01:57.203031Z",
     "iopub.status.busy": "2022-09-07T19:01:57.202604Z",
     "iopub.status.idle": "2022-09-07T19:03:55.374238Z",
     "shell.execute_reply": "2022-09-07T19:03:55.373068Z",
     "shell.execute_reply.started": "2022-09-07T19:01:57.202997Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "result_test_df = pd.read_csv('1661892619_9579706_test_file.csv', names=['reviews'])\n",
    "basic_cleaning(result_test_df)\n",
    "tokenize_data(result_test_df)\n",
    "result_test_df['stop_words_cleaned'] = result_test_df.words.apply(lambda word_list: remove_stopwords(word_list))\n",
    "result_test_df['lemma_word'] = result_test_df.stop_words_cleaned.apply(lambda word_list: find_lemma_word(word_list))\n",
    "result_test_df['cleaned_review'] = result_test_df.lemma_word.apply(lambda review_list: \" \".join(review_list))\n",
    "unlabeled_test_feat_matrix = selected_tf_vectorizer.transform(result_test_df['cleaned_review']).toarray()\n",
    "unlabeled_test_feat_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:04:33.636245Z",
     "iopub.status.busy": "2022-09-07T19:04:33.634928Z",
     "iopub.status.idle": "2022-09-07T19:04:46.405924Z",
     "shell.execute_reply": "2022-09-07T19:04:46.404674Z",
     "shell.execute_reply.started": "2022-09-07T19:04:33.636181Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "unlabled_weighted_data = generate_chi_square_weighted_features(unlabeled_test_feat_matrix, chi_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predicted_sentiments = []\n",
    "print(len(unlabled_weighted_data))\n",
    "for i in range(0,len(unlabled_weighted_data)):\n",
    "    # print(i)\n",
    "    predicted_sentiments.append(find_majority_and_predict(weighted_selected_feat,unlabled_weighted_data[i], train_sentiments_array, 23))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(predicted_sentiments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:09:46.720923Z",
     "iopub.status.busy": "2022-09-07T19:09:46.720426Z",
     "iopub.status.idle": "2022-09-07T19:09:46.745897Z",
     "shell.execute_reply": "2022-09-07T19:09:46.744737Z",
     "shell.execute_reply.started": "2022-09-07T19:09:46.720883Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "outfile = open('./results.csv','w')\n",
    "out = csv.writer(outfile)\n",
    "out.writerows(map(lambda x: [x], predicted_sentiments))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}