{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:29:28.319045Z",
     "iopub.status.busy": "2022-09-07T20:29:28.318572Z",
     "iopub.status.idle": "2022-09-07T20:29:28.603267Z",
     "shell.execute_reply": "2022-09-07T20:29:28.602081Z",
     "shell.execute_reply.started": "2022-09-07T20:29:28.319011Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('1661892619_92027_train_file.csv',names=[\"sentiments\", \"reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:29:29.703652Z",
     "iopub.status.busy": "2022-09-07T20:29:29.703190Z",
     "iopub.status.idle": "2022-09-07T20:29:30.930710Z",
     "shell.execute_reply": "2022-09-07T20:29:30.929762Z",
     "shell.execute_reply.started": "2022-09-07T20:29:29.703595Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def basic_cleaning(data_frame):\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.lower())\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.translate(str.maketrans('', '', string.punctuation)))\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: review.translate(str.maketrans('', '', string.digits)))\n",
    "    data_frame['reviews'] = data_frame.reviews.apply(lambda review: re.sub(\"r[^a-z]\",'',review))\n",
    "\n",
    "    \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_data(data_frame):\n",
    "    data_frame['words'] = data_frame.reviews.apply(lambda review: nltk.word_tokenize(review))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(list):\n",
    "    stop_words_removed = []\n",
    "    for i in list:\n",
    "        if i not in stopwords:\n",
    "            stop_words_removed.append(i)\n",
    "    return stop_words_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:29:39.969816Z",
     "iopub.status.busy": "2022-09-07T20:29:39.969420Z",
     "iopub.status.idle": "2022-09-07T20:30:27.157526Z",
     "shell.execute_reply": "2022-09-07T20:30:27.155569Z",
     "shell.execute_reply.started": "2022-09-07T20:29:39.969778Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')\n",
    "#tag part of speech to get more accurate word during lemmatizaton\n",
    "def tag_pos(list_of_words):\n",
    "    return nltk.pos_tag(list_of_words)\n",
    "\n",
    "#extraction of lemma words after pos taggin \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "def find_lemma_word(word):\n",
    "    lemma_words=[]\n",
    "    words_with_pos = tag_pos(word)\n",
    "    for word in words_with_pos:\n",
    "        if word[1].startswith('NN'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='n'))\n",
    "        elif word[1].startswith('VB'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='v'))\n",
    "        elif word[1].startswith('JJ'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='a'))\n",
    "        elif word[1].startswith('RB'):\n",
    "            lemma_words.append(lemmatizer.lemmatize(word[0],pos='r'))\n",
    "        else:\n",
    "            lemma_words.append(word[0])\n",
    "            \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:30:27.161877Z",
     "iopub.status.busy": "2022-09-07T20:30:27.161403Z",
     "iopub.status.idle": "2022-09-07T20:32:05.436075Z",
     "shell.execute_reply": "2022-09-07T20:32:05.434776Z",
     "shell.execute_reply.started": "2022-09-07T20:30:27.161838Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "basic_cleaning(train_data)\n",
    "tokenize_data(train_data)\n",
    "train_data['stop_words_cleaned'] = train_data.words.apply(lambda word_list: remove_stopwords(word_list))\n",
    "train_data['lemma_word'] = train_data.stop_words_cleaned.apply(lambda word_list: find_lemma_word(word_list))\n",
    "train_data['cleaned_review'] = train_data.lemma_word.apply(lambda review_list: \" \".join(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:05.439219Z",
     "iopub.status.busy": "2022-09-07T20:32:05.438473Z",
     "iopub.status.idle": "2022-09-07T20:32:05.495295Z",
     "shell.execute_reply": "2022-09-07T20:32:05.494068Z",
     "shell.execute_reply.started": "2022-09-07T20:32:05.439178Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   sentiments                                            reviews  \\\n0          -1  eat at fioris they said  youll like it they sa...   \n1          -1  i just dont understand the appeal  ive tried t...   \n2           1  this is my go to place foa really good beef en...   \n3          -1  not impressed when i ordered the oyako bowl th...   \n4          -1  this is the first time evei wrote a bad review...   \n\n                                               words  \\\n0  [eat, at, fioris, they, said, youll, like, it,...   \n1  [i, just, dont, understand, the, appeal, ive, ...   \n2  [this, is, my, go, to, place, foa, really, goo...   \n3  [not, impressed, when, i, ordered, the, oyako,...   \n4  [this, is, the, first, time, evei, wrote, a, b...   \n\n                                  stop_words_cleaned  \\\n0  [eat, fioris, said, youll, like, saidnnis, con...   \n1  [dont, understand, appeal, ive, tried, place, ...   \n2  [go, place, foa, really, good, beef, enchilada...   \n3  [impressed, ordered, oyako, bowl, conversation...   \n4  [first, time, evei, wrote, bad, review, frustr...   \n\n                                          lemma_word  \\\n0  [eat, fioris, say, youll, like, saidnnis, conv...   \n1  [dont, understand, appeal, ive, tried, place, ...   \n2  [go, place, foa, really, good, beef, enchilada...   \n3  [impressed, order, oyako, bowl, conversation, ...   \n4  [first, time, evei, write, bad, review, frustr...   \n\n                                      cleaned_review  \n0  eat fioris say youll like saidnnis convenientl...  \n1  dont understand appeal ive tried place twice t...  \n2  go place foa really good beef enchilada red sa...  \n3  impressed order oyako bowl conversation go som...  \n4  first time evei write bad review frustrate her...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiments</th>\n      <th>reviews</th>\n      <th>words</th>\n      <th>stop_words_cleaned</th>\n      <th>lemma_word</th>\n      <th>cleaned_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>eat at fioris they said  youll like it they sa...</td>\n      <td>[eat, at, fioris, they, said, youll, like, it,...</td>\n      <td>[eat, fioris, said, youll, like, saidnnis, con...</td>\n      <td>[eat, fioris, say, youll, like, saidnnis, conv...</td>\n      <td>eat fioris say youll like saidnnis convenientl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1</td>\n      <td>i just dont understand the appeal  ive tried t...</td>\n      <td>[i, just, dont, understand, the, appeal, ive, ...</td>\n      <td>[dont, understand, appeal, ive, tried, place, ...</td>\n      <td>[dont, understand, appeal, ive, tried, place, ...</td>\n      <td>dont understand appeal ive tried place twice t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>this is my go to place foa really good beef en...</td>\n      <td>[this, is, my, go, to, place, foa, really, goo...</td>\n      <td>[go, place, foa, really, good, beef, enchilada...</td>\n      <td>[go, place, foa, really, good, beef, enchilada...</td>\n      <td>go place foa really good beef enchilada red sa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1</td>\n      <td>not impressed when i ordered the oyako bowl th...</td>\n      <td>[not, impressed, when, i, ordered, the, oyako,...</td>\n      <td>[impressed, ordered, oyako, bowl, conversation...</td>\n      <td>[impressed, order, oyako, bowl, conversation, ...</td>\n      <td>impressed order oyako bowl conversation go som...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1</td>\n      <td>this is the first time evei wrote a bad review...</td>\n      <td>[this, is, the, first, time, evei, wrote, a, b...</td>\n      <td>[first, time, evei, wrote, bad, review, frustr...</td>\n      <td>[first, time, evei, write, bad, review, frustr...</td>\n      <td>first time evei write bad review frustrate her...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature subset Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:11.159062Z",
     "iopub.status.busy": "2022-09-07T20:32:11.158649Z",
     "iopub.status.idle": "2022-09-07T20:32:11.178064Z",
     "shell.execute_reply": "2022-09-07T20:32:11.176957Z",
     "shell.execute_reply.started": "2022-09-07T20:32:11.159019Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_review, test_review, train_sentiment, test_sentiment =train_test_split(train_data.cleaned_review,train_data.sentiments,\n",
    "                                                                             shuffle=True, ## shuffel to avoide sequential classes\n",
    "                                                                             random_state=0,\n",
    "                                                                             stratify=train_data.sentiments, ## create a balanced sample based on the target variable\n",
    "                                                                             train_size=.85) ## size of our training split testing split will be 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:11.924779Z",
     "iopub.status.busy": "2022-09-07T20:32:11.924343Z",
     "iopub.status.idle": "2022-09-07T20:32:17.315920Z",
     "shell.execute_reply": "2022-09-07T20:32:17.314576Z",
     "shell.execute_reply.started": "2022-09-07T20:32:11.924743Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(min_df=.0095, ## term must be present in atleast 0.95% of the document\n",
    "                                use_idf=False, ## creating a dictionary for now\n",
    "                                ngram_range=(1,2)) ## adding uni and bigram\n",
    "feature_matrix = tf_vectorizer.fit_transform(train_review)\n",
    "feature_array = feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T20:32:17.319084Z",
     "iopub.status.busy": "2022-09-07T20:32:17.318121Z",
     "iopub.status.idle": "2022-09-07T20:32:17.418220Z",
     "shell.execute_reply": "2022-09-07T20:32:17.416994Z",
     "shell.execute_reply.started": "2022-09-07T20:32:17.319042Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['able', 'absolutely', 'accommodate', 'across', 'act', 'actual',\n       'actually', 'add', 'admit', 'afte', 'aftea', 'aftei', 'afternoon',\n       'aftethe', 'ago', 'agree', 'ahead', 'airport', 'allow', 'almost',\n       'alone', 'along', 'already', 'also', 'although', 'always', 'amaze',\n       'amazing', 'ambiance', 'american', 'amount', 'anothe', 'anyone',\n       'anything', 'anyway', 'anywhere', 'apologize', 'apparently',\n       'appointment', 'appreciate', 'area', 'arent', 'arizona', 'around',\n       'arrive', 'ask', 'ask foa', 'ate', 'atmosphere', 'attention',\n       'attentive', 'attitude', 'authentic', 'available', 'average',\n       'avoid', 'away', 'awesome', 'awful', 'az', 'baand', 'back',\n       'bacon', 'bad', 'bag', 'bar', 'barely', 'base', 'basic',\n       'basically', 'bathroom', 'bbq', 'bean', 'beat', 'beautiful',\n       'become', 'bed', 'beef', 'beer', 'begin', 'behind', 'believe',\n       'best', 'bettethan', 'beyond', 'big', 'bill', 'birthday', 'bit',\n       'bite', 'black', 'bland', 'block', 'blue', 'book', 'bottle',\n       'bottom', 'bowl', 'box', 'boy', 'boyfriend', 'bread', 'break',\n       'breakfast', 'bring', 'brown', 'buck', 'buffet', 'build',\n       'building', 'bunch', 'burger', 'burrito', 'business', 'busy',\n       'buy', 'cafe', 'cake', 'call', 'cant', 'car', 'card', 'care',\n       'carry', 'case', 'cash', 'catch', 'cause', 'certainly', 'chain',\n       'chance', 'change', 'charge', 'charlotte', 'cheap', 'check',\n       'cheese', 'chef', 'chicken', 'child', 'chili', 'chinese',\n       'chinese food', 'chip', 'chocolate', 'choice', 'choose', 'city',\n       'class', 'clean', 'clearly', 'close', 'club', 'coffee', 'cold',\n       'combo', 'come', 'come back', 'comfortable', 'comment', 'company',\n       'compare', 'complain', 'complaint', 'complete', 'completely',\n       'consider', 'contact', 'continue', 'conversation', 'cook', 'cool',\n       'corn', 'cost', 'could', 'could get', 'couldnt', 'country',\n       'couple', 'coupon', 'course', 'cover', 'crab', 'crap', 'crave',\n       'crazy', 'cream', 'credit', 'crispy', 'crowd', 'crust', 'cup',\n       'customer', 'customeservice', 'cut', 'cute', 'damn', 'dark',\n       'date', 'day', 'deal', 'decent', 'decide', 'definitely', 'deli',\n       'delicious', 'deliver', 'delivery', 'desert', 'desk', 'despite',\n       'dessert', 'didnt', 'didnt even', 'didnt want', 'die', 'different',\n       'difficult', 'din', 'dip', 'dirty', 'disappoint', 'disappointed',\n       'discount', 'disgust', 'dish', 'do', 'doesnt', 'dog', 'dollar',\n       'dont', 'dont get', 'dont know', 'dont think', 'doubt', 'downtown',\n       'dress', 'drink', 'drive', 'drop', 'dry', 'due', 'early', 'easily',\n       'east', 'easy', 'eat', 'eaten', 'egg', 'else', 'elsewhere',\n       'employee', 'empty', 'end', 'enjoy', 'enough', 'entire', 'entree',\n       'especially', 'establishment', 'etc', 'evehad', 'even',\n       'even though', 'event', 'every', 'every time', 'everyone',\n       'everything', 'exactly', 'excellent', 'except', 'excite', 'expect',\n       'expectation', 'expensive', 'experience', 'explain', 'extra',\n       'extremely', 'eye', 'face', 'fact', 'fairly', 'fall', 'family',\n       'fan', 'fantastic', 'fast', 'favorite', 'fee', 'feel', 'feel like',\n       'felt', 'felt like', 'figure', 'fill', 'finally', 'find', 'fine',\n       'finish', 'first', 'first time', 'fish', 'fit', 'five', 'fix',\n       'flavor', 'flavorful', 'fly', 'fo', 'foa', 'foabout', 'foan',\n       'foit', 'folk', 'follow', 'folunch', 'fome', 'fomy', 'food',\n       'food good', 'food great', 'food service', 'foot', 'forget',\n       'forward', 'fosome', 'fothat', 'fothe', 'fothe first', 'fothem',\n       'fothis', 'fous', 'fowhat', 'foyears', 'foyou', 'free', 'french',\n       'frequent', 'fresh', 'friday', 'fried', 'friend', 'friendly',\n       'front', 'front desk', 'frozen', 'fry', 'full', 'fun', 'game',\n       'garden', 'garlic', 'general', 'generally', 'get', 'gift', 'girl',\n       'give', 'give place', 'give us', 'glad', 'glass', 'go', 'go back',\n       'go get', 'good', 'good food', 'good service', 'good thing',\n       'grab', 'greasy', 'great', 'great food', 'great place',\n       'great service', 'green', 'greet', 'grill', 'grocery', 'gross',\n       'ground', 'group', 'grow', 'guess', 'guest', 'guy', 'hadnt',\n       'half', 'hand', 'handle', 'hang', 'happen', 'happy', 'hard',\n       'hate', 'havent', 'head', 'heard', 'heat', 'hell', 'help',\n       'helpful', 'high', 'highly', 'highly recommend', 'hit', 'hold',\n       'hole', 'home', 'honest', 'honestly', 'hop', 'hope', 'horrible',\n       'hostess', 'hot', 'hotel', 'hour', 'house', 'howevethe', 'huge',\n       'hungry', 'husband', 'ice', 'ice cream', 'id', 'idea', 'ignore',\n       'ill', 'im', 'im sure', 'imagine', 'immediately', 'impress',\n       'impressed', 'include', 'incredibly', 'inform', 'ingredient',\n       'inside', 'instead', 'interest', 'isnt', 'issue', 'italian',\n       'item', 'ive', 'ive evehad', 'job', 'joint', 'joke', 'keep', 'kid',\n       'kind', 'kinda', 'kitchen', 'know', 'knowledgeable', 'la', 'lack',\n       'lady', 'large', 'last', 'last night', 'last time', 'late',\n       'learn', 'least', 'leave', 'less', 'let', 'lettuce', 'level',\n       'life', 'light', 'like', 'like place', 'line', 'list', 'listen',\n       'literally', 'little', 'live', 'local', 'locate', 'location',\n       'long', 'long time', 'look', 'look foa', 'look forward',\n       'look like', 'lose', 'lot', 'loud', 'love', 'love place', 'low',\n       'lunch', 'main', 'make', 'make feel', 'make sure', 'mall', 'man',\n       'manage', 'management', 'many', 'market', 'may', 'maybe', 'meal',\n       'mean', 'meat', 'mediocre', 'medium', 'meet', 'mention', 'menu',\n       'mess', 'mexican', 'mexican food', 'middle', 'might', 'mile',\n       'min', 'mind', 'mine', 'minute', 'miss', 'mistake', 'mix', 'mixed',\n       'mom', 'moment', 'money', 'month', 'morning', 'mostly', 'move',\n       'much', 'mushroom', 'music', 'must', 'name', 'nearly', 'need',\n       'negative', 'neighborhood', 'nevebeen', 'nevego', 'nevehad', 'new',\n       'next', 'next time', 'ni', 'nice', 'night', 'nni', 'nnmy', 'nnthe',\n       'nnthis', 'nnwe', 'none', 'noodle', 'normally', 'note', 'nothing',\n       'notice', 'nthe', 'oa', 'obviously', 'occasion', 'offer', 'office',\n       'often', 'oh', 'oil', 'ok', 'okay', 'old', 'olive', 'one',\n       'one best', 'onion', 'online', 'open', 'opinion', 'option',\n       'order', 'ordered', 'ordethe', 'othe', 'others', 'otherwise',\n       'othethan', 'oufood', 'outable', 'outside', 'ouwaitress', 'ove',\n       'overall', 'overprice', 'ovethe', 'oveto', 'owner', 'pack', 'park',\n       'parking', 'part', 'party', 'pas', 'pass', 'past', 'pasta',\n       'patio', 'pay', 'people', 'perfect', 'perfectly', 'perhaps',\n       'person', 'phoenix', 'phone', 'pick', 'picture', 'piece',\n       'pittsburgh', 'pizza', 'place', 'place get', 'place go', 'plain',\n       'plan', 'plate', 'play', 'pleasant', 'please', 'plenty', 'plus',\n       'pm', 'point', 'pool', 'pop', 'pork', 'portion', 'positive',\n       'possible', 'post', 'potato', 'prepare', 'pretty', 'pretty good',\n       'pretty much', 'previous', 'price', 'pricey', 'probably',\n       'problem', 'process', 'product', 'professional', 'provide', 'pull',\n       'purchase', 'put', 'quality', 'question', 'quick', 'quickly',\n       'quite', 'rare', 'rate', 'read', 'ready', 'real', 'realize',\n       'really', 'really good', 'really like', 'reason', 'reasonable',\n       'receive', 'recently', 'recommend', 'recommend place', 'red',\n       'refill', 'refuse', 'remind', 'replace', 'request', 'reservation',\n       'rest', 'restaurant', 'return', 'review', 'rib', 'rice',\n       'ridiculous', 'right', 'ring', 'road', 'rock', 'roll', 'room',\n       'rude', 'run', 'rush', 'sad', 'salad', 'sale', 'salmon', 'salon',\n       'salsa', 'salt', 'salty', 'sandwich', 'sat', 'saturday', 'sauce',\n       'sausage', 'save', 'saw', 'say', 'say would', 'schedule', 'school',\n       'scottsdale', 'seafood', 'search', 'season', 'seat', 'second',\n       'section', 'see', 'seem', 'seem like', 'selection', 'sell', 'send',\n       'sense', 'seriously', 'serve', 'server', 'service', 'service good',\n       'service great', 'set', 'several', 'several time', 'share', 'shop',\n       'short', 'shot', 'show', 'shrimp', 'sick', 'side', 'sign',\n       'simple', 'simply', 'since', 'single', 'sit', 'situation', 'size',\n       'slice', 'slightly', 'slow', 'small', 'smell', 'smile', 'smoke',\n       'soda', 'soft', 'soggy', 'someone', 'something', 'sometimes',\n       'somewhere', 'son', 'soon', 'sorry', 'sort', 'sound', 'soup',\n       'space', 'speak', 'special', 'spend', 'spice', 'spicy', 'sport',\n       'spot', 'st', 'staff', 'staff friendly', 'stand', 'standard',\n       'star', 'start', 'state', 'stay', 'steak', 'step', 'stick',\n       'still', 'stop', 'store', 'straight', 'street', 'strip', 'stuff',\n       'style', 'suck', 'suggest', 'sunday', 'suppose', 'sure',\n       'surprise', 'sushi', 'sweet', 'table', 'taco', 'take', 'take care',\n       'talk', 'taste', 'taste like', 'tasty', 'tea', 'tell', 'tell us',\n       'terrible', 'th', 'thank', 'thanks', 'thats', 'theifood', 'there',\n       'theyre', 'theyve', 'thick', 'thin', 'thing', 'think', 'though',\n       'thought', 'three', 'throw', 'time', 'time go', 'tiny', 'tip',\n       'tire', 'toast', 'today', 'told', 'tomato', 'ton', 'tonight',\n       'top', 'tortilla', 'total', 'totally', 'touch', 'town', 'travel',\n       'treat', 'tried', 'trip', 'true', 'truly', 'trust', 'try', 'turn',\n       'tv', 'twice', 'two', 'type', 'typical', 'understand',\n       'unfortunately', 'unique', 'unless', 'update', 'upon', 'us', 'use',\n       'usually', 'valley', 'value', 'variety', 'vegetable', 'vegetarian',\n       'veggie', 'view', 'visit', 'wait', 'wait staff', 'waitress',\n       'walk', 'wall', 'want', 'wanted', 'warm', 'wash', 'wasnt', 'waste',\n       'watch', 'way', 'wed', 'week', 'weekend', 'weird', 'welcome',\n       'well', 'werent', 'weve', 'whats', 'white', 'whole', 'wife',\n       'window', 'wine', 'wing', 'wish', 'within', 'without', 'woman',\n       'wonderful', 'wont', 'word', 'work', 'world', 'worth', 'would',\n       'would go', 'would recommend', 'wouldnt', 'wow', 'wrap', 'write',\n       'wrong', 'yeah', 'year', 'year ago', 'yelp', 'yes', 'yet', 'youd',\n       'youll', 'young', 'youre', 'youve', 'yummy'], dtype=object)"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = tf_vectorizer.get_feature_names_out()\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Select K-best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "vocab_list = tf_vectorizer.get_feature_names_out()\n",
    "select_k_best = SelectKBest(score_func=chi2, k= int(len(vocab_list)*.104))\n",
    "train_sentiment_np_array = np.array(train_sentiment)\n",
    "select_k_best.fit(feature_array, train_sentiment_np_array)\n",
    "mask = select_k_best.get_support()\n",
    "k_best_feature = vocab_list[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "data": {
      "text/plain": "(99,)"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_best_feature.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "selected_tf_vectorizer = TfidfVectorizer(use_idf=True, vocabulary=k_best_feature, sublinear_tf=True, ngram_range=(1,2))\n",
    "selected_feat_array = selected_tf_vectorizer.fit_transform(train_review).toarray()\n",
    "selected_test_array = selected_tf_vectorizer.transform(test_review).toarray()\n",
    "observed_value_table = pd.DataFrame(selected_tf_vectorizer.get_feature_names_out(), columns=['features'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['always', 'amaze', 'amazing', 'ask', 'avoid', 'awesome', 'awful',\n       'az', 'bad', 'beautiful', 'best', 'bland', 'call', 'charge',\n       'charlotte', 'cold', 'could', 'definitely', 'delicious', 'didnt',\n       'dirty', 'disappointed', 'disgust', 'dont', 'dry', 'easy', 'enjoy',\n       'excellent', 'family', 'fantastic', 'favorite', 'food great',\n       'fresh', 'friendly', 'fun', 'give', 'good', 'great', 'great food',\n       'great place', 'great service', 'gross', 'happy', 'helpful',\n       'highly', 'highly recommend', 'horrible', 'hotel', 'knowledgeable',\n       'lack', 'leave', 'little', 'love', 'love place', 'maybe',\n       'mediocre', 'mexican', 'minute', 'money', 'nice', 'nothing', 'ok',\n       'okay', 'one best', 'order', 'overprice', 'pay', 'perfect',\n       'phoenix', 'pittsburgh', 'recommend', 'rude', 'say', 'seem',\n       'selection', 'slow', 'sorry', 'spot', 'staff', 'table', 'taste',\n       'taste like', 'tell', 'terrible', 'think', 'two', 'unfortunately',\n       'us', 'valley', 'wait', 'waitress', 'wasnt', 'waste', 'wine',\n       'wonderful', 'wont', 'would', 'wouldnt', 'yummy'], dtype=object)"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tf_vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "test_sentiments_array = np.array(test_sentiment)\n",
    "train_sentiments_array = np.array(train_sentiment)\n",
    "# chi_sqare_table['max_chi_value'] = find_max_chi_val(chi_sqare_table['positiev_sentiments'], chi_sqare_table['negatiev_sentiments'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def calculate_neighbour(train_data, test_data):\n",
    "    \n",
    "    numpy_distances = np.array(euclidean_distances(train_data,[test_data]).flatten())\n",
    "    # storing inverse of distances\n",
    "    inv_distance = []\n",
    "    for i in numpy_distances:\n",
    "        # Divided exception avoided  by using a very small value\n",
    "        inv_distance.append(np.divide(1,max(i**3,.000000000000000000001**3)))\n",
    "\n",
    "    inv_distance = np.array(inv_distance)\n",
    "    indexes_by_shortest_dist = inv_distance.argsort() ## returns indexes from smallest to largest\n",
    "\n",
    "    return  np.flip(indexes_by_shortest_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_majority_and_predict(train_data, test_data, train_sentiments, K):\n",
    "    nearest_neighbors_indexes = calculate_neighbour(train_data, test_data)\n",
    "    sentiment_classes = []\n",
    "    for i in range(0,K):\n",
    "        sentiment_classes.append(train_sentiments[nearest_neighbors_indexes[i]])\n",
    "        \n",
    "    return max(sentiment_classes, key=sentiment_classes.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "def find_accuracy(predicted_sentiment, real_sentiment):\n",
    "    correct_prediction = 0\n",
    "    for i in range(0,len(predicted_sentiments)):\n",
    "        if predicted_sentiment[i] == real_sentiment[i]:\n",
    "            correct_prediction += 1\n",
    "    return np.divide(correct_prediction, len(predicted_sentiment))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_test_array))\n",
    "predicted_sentiments = []\n",
    "for i in range(0,len(selected_test_array)):\n",
    "    # print(i)\n",
    "    predicted_sentiments.append(find_majority_and_predict(selected_feat_array,selected_test_array[i], train_sentiments_array,22))\n",
    "score =  find_accuracy(predicted_sentiments, test_sentiments_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-07T21:57:54.970066Z",
     "iopub.status.idle": "2022-09-07T21:57:54.971113Z",
     "shell.execute_reply": "2022-09-07T21:57:54.970892Z",
     "shell.execute_reply.started": "2022-09-07T21:57:54.970867Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8318518518518518"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:01:57.203031Z",
     "iopub.status.busy": "2022-09-07T19:01:57.202604Z",
     "iopub.status.idle": "2022-09-07T19:03:55.374238Z",
     "shell.execute_reply": "2022-09-07T19:03:55.373068Z",
     "shell.execute_reply.started": "2022-09-07T19:01:57.202997Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_test_df = pd.read_csv('1661892619_9579706_test_file.csv', names=['reviews'])\n",
    "basic_cleaning(result_test_df)\n",
    "tokenize_data(result_test_df)\n",
    "result_test_df['stop_words_cleaned'] = result_test_df.words.apply(lambda word_list: remove_stopwords(word_list))\n",
    "result_test_df['lemma_word'] = result_test_df.stop_words_cleaned.apply(lambda word_list: find_lemma_word(word_list))\n",
    "result_test_df['cleaned_review'] = result_test_df.lemma_word.apply(lambda review_list: \" \".join(review_list))\n",
    "unlabeled_test_feat_matrix = selected_tf_vectorizer.transform(result_test_df['cleaned_review']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:04:25.997085Z",
     "iopub.status.busy": "2022-09-07T19:04:25.996619Z",
     "iopub.status.idle": "2022-09-07T19:04:26.005479Z",
     "shell.execute_reply": "2022-09-07T19:04:26.004136Z",
     "shell.execute_reply.started": "2022-09-07T19:04:25.997041Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(18000, 99)"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_test_feat_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "predicted_sentiments = []\n",
    "print(len(unlabeled_test_feat_matrix))\n",
    "for i in range(0,len(unlabeled_test_feat_matrix)):\n",
    "    # print(i)\n",
    "    predicted_sentiments.append(find_majority_and_predict(selected_feat_array,unlabeled_test_feat_matrix[i], train_sentiments_array, 22))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "18000"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:09:46.720923Z",
     "iopub.status.busy": "2022-09-07T19:09:46.720426Z",
     "iopub.status.idle": "2022-09-07T19:09:46.745897Z",
     "shell.execute_reply": "2022-09-07T19:09:46.744737Z",
     "shell.execute_reply.started": "2022-09-07T19:09:46.720883Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "outfile = open('./results.csv','w')\n",
    "out = csv.writer(outfile)\n",
    "out.writerows(map(lambda x: [x], predicted_sentiments))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}